{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa05ed2",
   "metadata": {},
   "source": [
    "## A Brief Introduction to Multilayer Perceptions\n",
    "A Perceptron is a single neuron model that was a precursor to larger neural networks. A perceptron is able to receive an single dimensional array of inputs and produce an single dimensional array of outputs that can be used for a variety of ML tasks, the most fundamental of which is classification. \n",
    "\n",
    "The building block for neural networks are artificial neurons. These are simple computational units that have weighted input signals and produce an output signal using an activation function. \n",
    "\n",
    "The weights attached to input signals can be likened to the coefficients of a regression equation. These weights represent the level of importance for each input. Consequently, changes in some inputs will have a larger effect on the output of the perceptron than others. Furthermore, each neuron will have a bias, a scalar weight that adjusts the final output of the neuron by a set amount, positive or negative.\n",
    "\n",
    "Weights are often initialized to small random values although more complex initialization schemes can be used. Larger weights tend to increase the complexity and fragility of the model, so it is desirable to keep weights small and regularization techniques can be used to decrease the risks of overfitting. \n",
    "\n",
    "### Activation\n",
    "\n",
    "The weighted inputs are summed and passed through an **activation function**, sometimes called a **transfer function**. It is called an activation function because it governs the threshold at which the neuron is \"activated\" and the strength of the output signal. The output of the function is limited in various ways, from application of the sigmoid function to bind values between 0 and 1, to hyperbolic tangent functions (Tanh) that binds values between -1 and 1, to ReLU (rectifier activation function) that binds values between 0 and another positive number. ReLU has been found to provide better results, generally, and this may be due to its mimicry of all-or-nothing type activation systems found in biological neurology. \n",
    "\n",
    "### Network of Neurons\n",
    "\n",
    "Neurons are arranged into networks of neurons. A row of neurons is called a **layer** and one network can have multiple layers. The architecture of the neurons in the network is often called the **network topology**. \n",
    "\n",
    "### Input of Visible Layers\n",
    "\n",
    "The bottom layer that takes input from a dataset is called the visible layer, because it is the exposed part of the network. Fundamentally, it is simply the series of initial inputs, where each input is the value from each column in the dataset for a given observation. \n",
    "\n",
    "### Hidden Layers\n",
    "Layers after the input layer are called **hidden layers** because they are not directly exposed to the input. Each input is effectively transformed through a weight and bias. \n",
    "\n",
    "### Output Layer\n",
    "\n",
    "The final hidden layer is called the output layer and is responsible for producing a value or vector of values that correspond to the format required for the problem. Here are some examples:\n",
    "- A regression problem may have a single output neuron and the neuron may have no activation function.\n",
    "- A binary classification problem may have a single output neuron and use a sigmoid activation function to output a value between 0 and 1 to represent the probability of predicting a value for the primary class. This can be turned into a crisp class value by using a threshold of 0.5 and snap values less than the threshold to 0 otherwise to 1. \n",
    "- A multiclass classification problem may have multiple neurons in the output layer, one for each class (e.g. three neurons for the three classes in the famous iris flowers classification problem). In this case, a softmax activation function may be used to output a probability of the network predicting each of the class values. Selecting the output with the highest probability can be used to produce a crisp class classification value. \n",
    "\n",
    "## Training Networks\n",
    "Once configured, the neural network must be trained on the dataset.\n",
    "\n",
    "### Data Preparation\n",
    "- Data must be numerical\n",
    "    - Categorical data can be converted via *one-hot encoding*.\n",
    "    - Text data will need to be converted via TF-IDF or some other NLP encoding technique.\n",
    "    - This same one hot encoding can be used on the output variable in classification problems with more than one class.\n",
    "- Data must be similarly scaled\n",
    "    - Min-max scaling (or any other scaling transformation) can be applied to the dataset to ensure consistency between inputs\n",
    "    \n",
    "### Stochastic Gradient Descent\n",
    "The classical training algorithm for neural networks is stochastic gradient descent. A single observation is exposed to the network, and the output is generated (called a **forward pass**). The output is compared to the expected output and an error is calculated. The error is propagated back through the network, one layer at a time, and the weights are updated according to the amount they contributed ot the error (known as **backpropagation**). One round of updating the network for the entire training dataset is called an **epoch**. A network may be trained for any number of epochs. \n",
    "\n",
    "### Weight Updates\n",
    "The weights can be updated from the errors calculated after each forward pass for each observation. This is called **online learning**. Alternatively, the errors can be aggregated across all results of all observations in the training set. This is called **batch learning** and is often more stable. \n",
    "\n",
    "Because datasets are so large, the size of the batch will be reduced before an update is completed. The amount that the weights are updated is controlled by a configuration parameter called the learning rate. This is also called the **step size** and controls the momentum at which a local minima is approached in order to reduce the risk of a convergence failure (where a local minima cannot be identified because each update overshoots a true local minima of the aggregated errors). \n",
    "- **Momentum** is a term that incorporates the properties from the previous weight update to allow the weights to continue to change in the same direction even when there is less error being calculated.\n",
    "- **Learning Rate Decay** is used to decrease the learning weight over epochs to allow the network to make large changes to the weights at the beginning and smaller fine tuning changes later in the training schedule.\n",
    "\n",
    "### Prediction\n",
    "Predictions are made by providing the input to the network and performing a forward-pass allowing it to generate an ouput that you can use as a prediction. Evaluating performance on a separate out-of-sample dataset (ideally randomly split from the original full dataset available) can alert to risks of overfitting.\n",
    "\n",
    "The network topology and the final set of weights is all that needs to persist from the research environment into the production environment to make novel predictions. \n",
    "\n",
    "## Developing a Simple Neural Network from the Pima Indians Onset of Diabetes Dataset\n",
    "\n",
    "This is a standard machine learning dataset available from the UCI Machine Learning repository. It describes patient medical record data for Pima Indians and whether they had an onset of diabetes within five years. It is a binary classification problem (onset of diabetes as 1 or not as 0). Below are the attributes for the dataset:\n",
    "1. Number of times pregnant.\n",
    "2. Plasma glucose concentration at 2 hours in an oral glucose tolerance test.\n",
    "3. Diastolic blood pressure (mm Hg).\n",
    "4. Triceps skin fold thickness (mm).\n",
    "5. 2-Hour serum insulin (mu U/ml).\n",
    "6. Body mass index.\n",
    "7. Diabetes pedigree function.\n",
    "8. Age (years).\n",
    "9. Class, onset of diabetes within five years.\n",
    "\n",
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adf72d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6da56488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.   , 148.   ,  72.   , ...,   0.627,  50.   ,   1.   ],\n",
       "       [  1.   ,  85.   ,  66.   , ...,   0.351,  31.   ,   0.   ],\n",
       "       [  8.   , 183.   ,  64.   , ...,   0.672,  32.   ,   1.   ],\n",
       "       ...,\n",
       "       [  5.   , 121.   ,  72.   , ...,   0.245,  30.   ,   0.   ],\n",
       "       [  1.   , 126.   ,  60.   , ...,   0.349,  47.   ,   1.   ],\n",
       "       [  1.   ,  93.   ,  70.   , ...,   0.315,  23.   ,   0.   ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the dataset\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af22c7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input (X) and output (y) variables\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d18b3a3",
   "metadata": {},
   "source": [
    "## Defining the Model\n",
    "\n",
    "Models in Keras are a sequence of layers. We create a `Sequential` model and add layers one at a time until we are happy with our network topology. The first thing to get right is to ensure the input layer has the right number of inputs. This can be specified when creating the first layer with the `input_dim` argument and setting it to `8` for the 8 input variables. \n",
    "\n",
    "How do we know the number of layers to use and their types? Trial and error experimentation. Generally, you need a network large enough to capture the structure of the problem. In this example, we will use a fully-connected network structure with three layers.\n",
    "\n",
    "Fully connected layers are defined using the `Dense` class. We can specify the number f neurons in the layer as the first argument and specify the activation function using the `activation` argument. We will use the rectifier (`relu`) activation function on the first two layers and the `sigmoid` activation function in the output layer. It used to be the case that sigmoid and `tanh` activation functions were preferred for all layers. These days, better performance is seen using the rectifier activation function. For the output layer, we can snap the classification by setting a threshold, in this case 0.5.\n",
    "\n",
    "The first hidden layer will have 12 neurons and expect 8 input variables (e.g. `input_dim=8`). The second hidden layer will have 8 neurons and finally the output layer will have 1 neuron to predict the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24473260",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3988b269",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "\n",
    "The backend will automatically choose the best way to represent the network for training and making predictions to run on my hardware. When compiling, we must specify some additional properties required when training the network:\n",
    "- The loss function to used to evaluate a set of weights\n",
    "- The optimizer used to serach through different weights \n",
    "- Any optional metrics we would like to collect and report\n",
    "\n",
    "In this case, we will use logarithmic loss, which for a binary classification problem is defined in Keras as `binary_crossentropy`. We will use the gradient descent algorithm `adam` as it is an efficient default. For metrics, we will capture and report the model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fcf4839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d83fe",
   "metadata": {},
   "source": [
    "## Fit the Model\n",
    "\n",
    "We have defined our model and compiled it for efficient computation. Now it is time to execute the model on some data. We can train on our loaded dat by calling the `fit()` function on the model.\n",
    "\n",
    "The training process will run for a fixed number of iterations through the dataset called epochs, that we must specify with the `epochs` argument. We can also set the number of instances that are evaluated before a weight update in the network is performed called the batch sixe and set using the `batch_size` argument. For this problem we will run a small number of epochs (150) and use a relatively small batch size of 16. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ea6408e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "48/48 [==============================] - 1s 963us/step - loss: 1.8499 - accuracy: 0.6302\n",
      "Epoch 2/150\n",
      "48/48 [==============================] - 0s 795us/step - loss: 1.3707 - accuracy: 0.6471\n",
      "Epoch 3/150\n",
      "48/48 [==============================] - 0s 771us/step - loss: 1.1742 - accuracy: 0.6302\n",
      "Epoch 4/150\n",
      "48/48 [==============================] - 0s 844us/step - loss: 0.9752 - accuracy: 0.6484\n",
      "Epoch 5/150\n",
      "48/48 [==============================] - 0s 816us/step - loss: 0.8909 - accuracy: 0.6484\n",
      "Epoch 6/150\n",
      "48/48 [==============================] - 0s 891us/step - loss: 0.8530 - accuracy: 0.6615\n",
      "Epoch 7/150\n",
      "48/48 [==============================] - 0s 886us/step - loss: 0.7945 - accuracy: 0.6771\n",
      "Epoch 8/150\n",
      "48/48 [==============================] - 0s 919us/step - loss: 0.7884 - accuracy: 0.6654\n",
      "Epoch 9/150\n",
      "48/48 [==============================] - 0s 891us/step - loss: 0.7522 - accuracy: 0.6706\n",
      "Epoch 10/150\n",
      "48/48 [==============================] - 0s 923us/step - loss: 0.7055 - accuracy: 0.6901\n",
      "Epoch 11/150\n",
      "48/48 [==============================] - 0s 919us/step - loss: 0.6970 - accuracy: 0.6693\n",
      "Epoch 12/150\n",
      "48/48 [==============================] - 0s 760us/step - loss: 0.6695 - accuracy: 0.6745\n",
      "Epoch 13/150\n",
      "48/48 [==============================] - 0s 721us/step - loss: 0.6823 - accuracy: 0.6680\n",
      "Epoch 14/150\n",
      "48/48 [==============================] - 0s 713us/step - loss: 0.6571 - accuracy: 0.6771\n",
      "Epoch 15/150\n",
      "48/48 [==============================] - 0s 809us/step - loss: 0.6650 - accuracy: 0.6797\n",
      "Epoch 16/150\n",
      "48/48 [==============================] - 0s 882us/step - loss: 0.6840 - accuracy: 0.6719\n",
      "Epoch 17/150\n",
      "48/48 [==============================] - 0s 878us/step - loss: 0.6405 - accuracy: 0.6719\n",
      "Epoch 18/150\n",
      "48/48 [==============================] - 0s 887us/step - loss: 0.6322 - accuracy: 0.6810\n",
      "Epoch 19/150\n",
      "48/48 [==============================] - 0s 910us/step - loss: 0.6237 - accuracy: 0.6758\n",
      "Epoch 20/150\n",
      "48/48 [==============================] - 0s 879us/step - loss: 0.6290 - accuracy: 0.6862\n",
      "Epoch 21/150\n",
      "48/48 [==============================] - 0s 896us/step - loss: 0.6357 - accuracy: 0.6771\n",
      "Epoch 22/150\n",
      "48/48 [==============================] - 0s 887us/step - loss: 0.6153 - accuracy: 0.6888\n",
      "Epoch 23/150\n",
      "48/48 [==============================] - 0s 865us/step - loss: 0.6242 - accuracy: 0.6888\n",
      "Epoch 24/150\n",
      "48/48 [==============================] - 0s 887us/step - loss: 0.6174 - accuracy: 0.6901\n",
      "Epoch 25/150\n",
      "48/48 [==============================] - 0s 866us/step - loss: 0.6097 - accuracy: 0.6875\n",
      "Epoch 26/150\n",
      "48/48 [==============================] - 0s 838us/step - loss: 0.6116 - accuracy: 0.7122\n",
      "Epoch 27/150\n",
      "48/48 [==============================] - 0s 871us/step - loss: 0.6123 - accuracy: 0.7005\n",
      "Epoch 28/150\n",
      "48/48 [==============================] - 0s 732us/step - loss: 0.6093 - accuracy: 0.6992\n",
      "Epoch 29/150\n",
      "48/48 [==============================] - 0s 725us/step - loss: 0.6056 - accuracy: 0.7057\n",
      "Epoch 30/150\n",
      "48/48 [==============================] - 0s 856us/step - loss: 0.6042 - accuracy: 0.6992\n",
      "Epoch 31/150\n",
      "48/48 [==============================] - 0s 817us/step - loss: 0.5924 - accuracy: 0.7057\n",
      "Epoch 32/150\n",
      "48/48 [==============================] - 0s 880us/step - loss: 0.5991 - accuracy: 0.7174\n",
      "Epoch 33/150\n",
      "48/48 [==============================] - 0s 907us/step - loss: 0.6181 - accuracy: 0.6823\n",
      "Epoch 34/150\n",
      "48/48 [==============================] - 0s 848us/step - loss: 0.5865 - accuracy: 0.7018\n",
      "Epoch 35/150\n",
      "48/48 [==============================] - 0s 906us/step - loss: 0.5933 - accuracy: 0.7174\n",
      "Epoch 36/150\n",
      "48/48 [==============================] - 0s 836us/step - loss: 0.5942 - accuracy: 0.7031\n",
      "Epoch 37/150\n",
      "48/48 [==============================] - 0s 742us/step - loss: 0.5883 - accuracy: 0.7135\n",
      "Epoch 38/150\n",
      "48/48 [==============================] - 0s 819us/step - loss: 0.5963 - accuracy: 0.6979\n",
      "Epoch 39/150\n",
      "48/48 [==============================] - 0s 825us/step - loss: 0.5952 - accuracy: 0.7122\n",
      "Epoch 40/150\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.5883 - accuracy: 0.7031\n",
      "Epoch 41/150\n",
      "48/48 [==============================] - 0s 967us/step - loss: 0.5951 - accuracy: 0.7057\n",
      "Epoch 42/150\n",
      "48/48 [==============================] - 0s 879us/step - loss: 0.5886 - accuracy: 0.7174\n",
      "Epoch 43/150\n",
      "48/48 [==============================] - 0s 864us/step - loss: 0.5825 - accuracy: 0.6901\n",
      "Epoch 44/150\n",
      "48/48 [==============================] - 0s 847us/step - loss: 0.5812 - accuracy: 0.7031\n",
      "Epoch 45/150\n",
      "48/48 [==============================] - 0s 851us/step - loss: 0.5701 - accuracy: 0.7201\n",
      "Epoch 46/150\n",
      "48/48 [==============================] - 0s 883us/step - loss: 0.5747 - accuracy: 0.7227\n",
      "Epoch 47/150\n",
      "48/48 [==============================] - 0s 946us/step - loss: 0.5816 - accuracy: 0.7201\n",
      "Epoch 48/150\n",
      "48/48 [==============================] - 0s 890us/step - loss: 0.5828 - accuracy: 0.7044\n",
      "Epoch 49/150\n",
      "48/48 [==============================] - 0s 835us/step - loss: 0.5782 - accuracy: 0.7096\n",
      "Epoch 50/150\n",
      "48/48 [==============================] - 0s 851us/step - loss: 0.5746 - accuracy: 0.7109\n",
      "Epoch 51/150\n",
      "48/48 [==============================] - 0s 814us/step - loss: 0.5769 - accuracy: 0.7331\n",
      "Epoch 52/150\n",
      "48/48 [==============================] - 0s 837us/step - loss: 0.5732 - accuracy: 0.7227\n",
      "Epoch 53/150\n",
      "48/48 [==============================] - 0s 866us/step - loss: 0.5697 - accuracy: 0.7292\n",
      "Epoch 54/150\n",
      "48/48 [==============================] - 0s 870us/step - loss: 0.5731 - accuracy: 0.7083\n",
      "Epoch 55/150\n",
      "48/48 [==============================] - 0s 881us/step - loss: 0.5637 - accuracy: 0.7266\n",
      "Epoch 56/150\n",
      "48/48 [==============================] - 0s 848us/step - loss: 0.5654 - accuracy: 0.7240\n",
      "Epoch 57/150\n",
      "48/48 [==============================] - 0s 851us/step - loss: 0.5628 - accuracy: 0.7214\n",
      "Epoch 58/150\n",
      "48/48 [==============================] - 0s 807us/step - loss: 0.5645 - accuracy: 0.7370\n",
      "Epoch 59/150\n",
      "48/48 [==============================] - 0s 885us/step - loss: 0.5675 - accuracy: 0.7201\n",
      "Epoch 60/150\n",
      "48/48 [==============================] - 0s 885us/step - loss: 0.5713 - accuracy: 0.7135\n",
      "Epoch 61/150\n",
      "48/48 [==============================] - 0s 843us/step - loss: 0.5562 - accuracy: 0.7148\n",
      "Epoch 62/150\n",
      "48/48 [==============================] - 0s 802us/step - loss: 0.5640 - accuracy: 0.7188\n",
      "Epoch 63/150\n",
      "48/48 [==============================] - 0s 787us/step - loss: 0.5744 - accuracy: 0.7083\n",
      "Epoch 64/150\n",
      "48/48 [==============================] - 0s 853us/step - loss: 0.5599 - accuracy: 0.7174\n",
      "Epoch 65/150\n",
      "48/48 [==============================] - 0s 866us/step - loss: 0.5601 - accuracy: 0.7240\n",
      "Epoch 66/150\n",
      "48/48 [==============================] - 0s 881us/step - loss: 0.5556 - accuracy: 0.7344\n",
      "Epoch 67/150\n",
      "48/48 [==============================] - 0s 893us/step - loss: 0.5595 - accuracy: 0.7188\n",
      "Epoch 68/150\n",
      "48/48 [==============================] - 0s 813us/step - loss: 0.5617 - accuracy: 0.7305\n",
      "Epoch 69/150\n",
      "48/48 [==============================] - 0s 732us/step - loss: 0.5570 - accuracy: 0.7448\n",
      "Epoch 70/150\n",
      "48/48 [==============================] - 0s 730us/step - loss: 0.5505 - accuracy: 0.7240\n",
      "Epoch 71/150\n",
      "48/48 [==============================] - 0s 719us/step - loss: 0.5525 - accuracy: 0.7396\n",
      "Epoch 72/150\n",
      "48/48 [==============================] - 0s 736us/step - loss: 0.5571 - accuracy: 0.7383\n",
      "Epoch 73/150\n",
      "48/48 [==============================] - 0s 725us/step - loss: 0.5531 - accuracy: 0.7318\n",
      "Epoch 74/150\n",
      "48/48 [==============================] - 0s 781us/step - loss: 0.5610 - accuracy: 0.7305\n",
      "Epoch 75/150\n",
      "48/48 [==============================] - 0s 796us/step - loss: 0.5460 - accuracy: 0.7435\n",
      "Epoch 76/150\n",
      "48/48 [==============================] - 0s 858us/step - loss: 0.5540 - accuracy: 0.7266\n",
      "Epoch 77/150\n",
      "48/48 [==============================] - 0s 837us/step - loss: 0.5457 - accuracy: 0.7331\n",
      "Epoch 78/150\n",
      "48/48 [==============================] - 0s 840us/step - loss: 0.5434 - accuracy: 0.7422\n",
      "Epoch 79/150\n",
      "48/48 [==============================] - 0s 862us/step - loss: 0.5507 - accuracy: 0.7318\n",
      "Epoch 80/150\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.5450 - accuracy: 0.7344\n",
      "Epoch 81/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 882us/step - loss: 0.5534 - accuracy: 0.7161\n",
      "Epoch 82/150\n",
      "48/48 [==============================] - 0s 863us/step - loss: 0.5548 - accuracy: 0.7253\n",
      "Epoch 83/150\n",
      "48/48 [==============================] - 0s 736us/step - loss: 0.5462 - accuracy: 0.7305\n",
      "Epoch 84/150\n",
      "48/48 [==============================] - 0s 706us/step - loss: 0.5383 - accuracy: 0.7370\n",
      "Epoch 85/150\n",
      "48/48 [==============================] - 0s 723us/step - loss: 0.5488 - accuracy: 0.7292\n",
      "Epoch 86/150\n",
      "48/48 [==============================] - 0s 752us/step - loss: 0.5405 - accuracy: 0.7331\n",
      "Epoch 87/150\n",
      "48/48 [==============================] - 0s 814us/step - loss: 0.5379 - accuracy: 0.7292\n",
      "Epoch 88/150\n",
      "48/48 [==============================] - 0s 755us/step - loss: 0.5469 - accuracy: 0.7370\n",
      "Epoch 89/150\n",
      "48/48 [==============================] - 0s 713us/step - loss: 0.5431 - accuracy: 0.7201\n",
      "Epoch 90/150\n",
      "48/48 [==============================] - 0s 742us/step - loss: 0.5346 - accuracy: 0.7357\n",
      "Epoch 91/150\n",
      "48/48 [==============================] - 0s 710us/step - loss: 0.5359 - accuracy: 0.7500\n",
      "Epoch 92/150\n",
      "48/48 [==============================] - 0s 722us/step - loss: 0.5395 - accuracy: 0.7383\n",
      "Epoch 93/150\n",
      "48/48 [==============================] - 0s 742us/step - loss: 0.5327 - accuracy: 0.7357\n",
      "Epoch 94/150\n",
      "48/48 [==============================] - 0s 705us/step - loss: 0.5463 - accuracy: 0.7279\n",
      "Epoch 95/150\n",
      "48/48 [==============================] - 0s 721us/step - loss: 0.5415 - accuracy: 0.7318\n",
      "Epoch 96/150\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.5394 - accuracy: 0.7396\n",
      "Epoch 97/150\n",
      "48/48 [==============================] - 0s 850us/step - loss: 0.5321 - accuracy: 0.7435\n",
      "Epoch 98/150\n",
      "48/48 [==============================] - 0s 831us/step - loss: 0.5420 - accuracy: 0.7383\n",
      "Epoch 99/150\n",
      "48/48 [==============================] - 0s 858us/step - loss: 0.5362 - accuracy: 0.7318\n",
      "Epoch 100/150\n",
      "48/48 [==============================] - 0s 838us/step - loss: 0.5502 - accuracy: 0.7383\n",
      "Epoch 101/150\n",
      "48/48 [==============================] - 0s 782us/step - loss: 0.5347 - accuracy: 0.7370\n",
      "Epoch 102/150\n",
      "48/48 [==============================] - 0s 734us/step - loss: 0.5476 - accuracy: 0.7422\n",
      "Epoch 103/150\n",
      "48/48 [==============================] - 0s 735us/step - loss: 0.5296 - accuracy: 0.7448\n",
      "Epoch 104/150\n",
      "48/48 [==============================] - 0s 732us/step - loss: 0.5288 - accuracy: 0.7487\n",
      "Epoch 105/150\n",
      "48/48 [==============================] - 0s 787us/step - loss: 0.5328 - accuracy: 0.7370\n",
      "Epoch 106/150\n",
      "48/48 [==============================] - 0s 802us/step - loss: 0.5282 - accuracy: 0.7435\n",
      "Epoch 107/150\n",
      "48/48 [==============================] - 0s 810us/step - loss: 0.5395 - accuracy: 0.7279\n",
      "Epoch 108/150\n",
      "48/48 [==============================] - 0s 786us/step - loss: 0.5374 - accuracy: 0.7318\n",
      "Epoch 109/150\n",
      "48/48 [==============================] - 0s 828us/step - loss: 0.5369 - accuracy: 0.7461\n",
      "Epoch 110/150\n",
      "48/48 [==============================] - 0s 773us/step - loss: 0.5400 - accuracy: 0.7240\n",
      "Epoch 111/150\n",
      "48/48 [==============================] - 0s 704us/step - loss: 0.5328 - accuracy: 0.7305\n",
      "Epoch 112/150\n",
      "48/48 [==============================] - 0s 733us/step - loss: 0.5253 - accuracy: 0.7344\n",
      "Epoch 113/150\n",
      "48/48 [==============================] - 0s 753us/step - loss: 0.5305 - accuracy: 0.7409\n",
      "Epoch 114/150\n",
      "48/48 [==============================] - 0s 888us/step - loss: 0.5266 - accuracy: 0.7526\n",
      "Epoch 115/150\n",
      "48/48 [==============================] - 0s 862us/step - loss: 0.5256 - accuracy: 0.7474\n",
      "Epoch 116/150\n",
      "48/48 [==============================] - 0s 869us/step - loss: 0.5285 - accuracy: 0.7409\n",
      "Epoch 117/150\n",
      "48/48 [==============================] - 0s 885us/step - loss: 0.5364 - accuracy: 0.7318\n",
      "Epoch 118/150\n",
      "48/48 [==============================] - 0s 899us/step - loss: 0.5440 - accuracy: 0.7357\n",
      "Epoch 119/150\n",
      "48/48 [==============================] - 0s 881us/step - loss: 0.5261 - accuracy: 0.7331\n",
      "Epoch 120/150\n",
      "48/48 [==============================] - 0s 990us/step - loss: 0.5235 - accuracy: 0.7448\n",
      "Epoch 121/150\n",
      "48/48 [==============================] - 0s 902us/step - loss: 0.5284 - accuracy: 0.7357\n",
      "Epoch 122/150\n",
      "48/48 [==============================] - 0s 868us/step - loss: 0.5261 - accuracy: 0.7552\n",
      "Epoch 123/150\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.5256 - accuracy: 0.7409\n",
      "Epoch 124/150\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.5180 - accuracy: 0.7409\n",
      "Epoch 125/150\n",
      "48/48 [==============================] - 0s 897us/step - loss: 0.5169 - accuracy: 0.7513\n",
      "Epoch 126/150\n",
      "48/48 [==============================] - 0s 874us/step - loss: 0.5263 - accuracy: 0.7396\n",
      "Epoch 127/150\n",
      "48/48 [==============================] - 0s 849us/step - loss: 0.5297 - accuracy: 0.7357\n",
      "Epoch 128/150\n",
      "48/48 [==============================] - 0s 866us/step - loss: 0.5177 - accuracy: 0.7461\n",
      "Epoch 129/150\n",
      "48/48 [==============================] - 0s 825us/step - loss: 0.5217 - accuracy: 0.7461\n",
      "Epoch 130/150\n",
      "48/48 [==============================] - 0s 717us/step - loss: 0.5251 - accuracy: 0.7357\n",
      "Epoch 131/150\n",
      "48/48 [==============================] - 0s 702us/step - loss: 0.5193 - accuracy: 0.7500\n",
      "Epoch 132/150\n",
      "48/48 [==============================] - 0s 753us/step - loss: 0.5302 - accuracy: 0.7344\n",
      "Epoch 133/150\n",
      "48/48 [==============================] - 0s 828us/step - loss: 0.5148 - accuracy: 0.7487\n",
      "Epoch 134/150\n",
      "48/48 [==============================] - 0s 863us/step - loss: 0.5199 - accuracy: 0.7409\n",
      "Epoch 135/150\n",
      "48/48 [==============================] - 0s 943us/step - loss: 0.5168 - accuracy: 0.7461\n",
      "Epoch 136/150\n",
      "48/48 [==============================] - 0s 856us/step - loss: 0.5164 - accuracy: 0.7370\n",
      "Epoch 137/150\n",
      "48/48 [==============================] - 0s 804us/step - loss: 0.5286 - accuracy: 0.7396\n",
      "Epoch 138/150\n",
      "48/48 [==============================] - 0s 830us/step - loss: 0.5222 - accuracy: 0.7396\n",
      "Epoch 139/150\n",
      "48/48 [==============================] - 0s 826us/step - loss: 0.5284 - accuracy: 0.7383\n",
      "Epoch 140/150\n",
      "48/48 [==============================] - 0s 871us/step - loss: 0.5111 - accuracy: 0.7487\n",
      "Epoch 141/150\n",
      "48/48 [==============================] - 0s 828us/step - loss: 0.5153 - accuracy: 0.7487\n",
      "Epoch 142/150\n",
      "48/48 [==============================] - 0s 877us/step - loss: 0.5205 - accuracy: 0.7474\n",
      "Epoch 143/150\n",
      "48/48 [==============================] - 0s 891us/step - loss: 0.5227 - accuracy: 0.7383\n",
      "Epoch 144/150\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.5129 - accuracy: 0.7461\n",
      "Epoch 145/150\n",
      "48/48 [==============================] - 0s 862us/step - loss: 0.5121 - accuracy: 0.7578\n",
      "Epoch 146/150\n",
      "48/48 [==============================] - 0s 872us/step - loss: 0.5069 - accuracy: 0.7552\n",
      "Epoch 147/150\n",
      "48/48 [==============================] - 0s 840us/step - loss: 0.5103 - accuracy: 0.7500\n",
      "Epoch 148/150\n",
      "48/48 [==============================] - 0s 876us/step - loss: 0.5070 - accuracy: 0.7617\n",
      "Epoch 149/150\n",
      "48/48 [==============================] - 0s 890us/step - loss: 0.5130 - accuracy: 0.7370\n",
      "Epoch 150/150\n",
      "48/48 [==============================] - 0s 825us/step - loss: 0.5121 - accuracy: 0.7526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe65a6db670>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=150, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb751d3",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "While we have an accuracy rating, it only applies to the training dataset. We do not have an out of sample dataset to work with. \n",
    "\n",
    "We can use the `evaluate()` function and pass it the same input/output used to train the model. This will generate a prediction for each input and output pair and collect scores, including the average loss and any metrics you have configured, such as accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6472b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 854us/step - loss: 0.5149 - accuracy: 0.7448\n",
      "Accuracy: 74.48\n"
     ]
    }
   ],
   "source": [
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X, y)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3df868",
   "metadata": {},
   "source": [
    "Neural networks are a stochastic algorithm, meaning that the same algorithm ont he same data can train a different model with different skill each time the code is run. This is a feature, not a bug. The variance in the performance of the model means that to get a reasomable approximation of how well your model is performing, you may need to fit it many times and calculate the average of the accuracy scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ab14ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_model_example(X=X, y=y, num=5):\n",
    "    accuracies = []\n",
    "    for i in range(num):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "        model.add(Dense(8, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        # compile the keras model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # fit the keras model on the dataset\n",
    "        model.fit(X, y, epochs=150, batch_size=16, verbose=0)\n",
    "        # evaluate the keras model\n",
    "        _, accuracy = model.evaluate(X, y)\n",
    "        accuracies.append(accuracy*100)\n",
    "    print(\"\\n\", f\"The average accuracy over {num} fits is: {round (sum(accuracies) / len(accuracies), 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13a3dd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 752us/step - loss: 0.4784 - accuracy: 0.7617\n",
      "24/24 [==============================] - 0s 826us/step - loss: 0.4536 - accuracy: 0.7943\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5311 - accuracy: 0.7513\n",
      "24/24 [==============================] - 0s 833us/step - loss: 0.4716 - accuracy: 0.7799\n",
      "24/24 [==============================] - 0s 915us/step - loss: 0.4923 - accuracy: 0.7656\n",
      "\n",
      " The average accuracy over 5 fits is: 77.06%\n"
     ]
    }
   ],
   "source": [
    "multiple_model_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50be26a",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "\n",
    "We can adapt the above example and use it to generate predictions on the training dataset, pretending it is a new dataset we have not seen before. Making predictions is as easy as calling the `predict()` function on the model. We are using a sigmoid activation function on the output layer, so the predictions will be a probability in the range between 0 and 1. We can easily convert them into a crisp binary prediction for this classification task by rounding them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d76e4ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 690us/step\n"
     ]
    }
   ],
   "source": [
    "# make probability predictions with the model\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd8637e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing the first 5 predictions:\n",
      "[0, 0, 1, 0, 1]\n",
      "There are 768 predictions made for this set.\n"
     ]
    }
   ],
   "source": [
    "print(\"Showing the first 5 predictions:\")\n",
    "print(rounded[:5])\n",
    "print(f\"There are {len(rounded)} predictions made for this set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73e373f",
   "metadata": {},
   "source": [
    "We can compare the predictions to the actual values in the target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f22e478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0] => 0 (expected 1)\n",
      "[1.0, 85.0, 66.0, 29.0, 0.0, 26.6, 0.351, 31.0] => 0 (expected 0)\n",
      "[8.0, 183.0, 64.0, 0.0, 0.0, 23.3, 0.672, 32.0] => 1 (expected 1)\n",
      "[1.0, 89.0, 66.0, 23.0, 94.0, 28.1, 0.167, 21.0] => 0 (expected 0)\n",
      "[0.0, 137.0, 40.0, 35.0, 168.0, 43.1, 2.288, 33.0] => 1 (expected 1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"%s => %d (expected %d)\" % (X[i].tolist(), rounded[i], y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40402087",
   "metadata": {},
   "source": [
    "## Considerations on Model Evaluation\n",
    "\n",
    "There are a lot of decisions to make when designing and configuring deep learning models. Most of these decisions must be resolved empirically through trial and error. As such, it is critically important to have a robust way to evaluate the performance of a neural network. The following three methods will be explored: \n",
    "- Automatic verification\n",
    "- Manual verification\n",
    "- *k*-fold cross-validation\n",
    "\n",
    "As many optimal topology structures have been developed through empirical evaluation, a simple approach is to begin by copying the structure of known optimal network topology and using heuristics. The best technique is to actually design small experiments and empirically evaluate options using real data. This includes high-level decisions like:\n",
    "- Number of layers\n",
    "- Size of layers\n",
    "- Type of layers\n",
    "\n",
    "And lower-level decisions like:\n",
    "- Choice of loss function\n",
    "- Choice of activation function\n",
    "- Choice of optimization procedure\n",
    "- Number of epochs\n",
    "\n",
    "## Splitting\n",
    "\n",
    "Large amounts of data combined with the complexity of neural networks result in exceptionally long training times. As such, it is typical to use a simple separation of data into training, validation, and test datasets. The use of splitting further reduces the risk of overfitting. \n",
    "\n",
    "### Using an Automatic Verification\n",
    "\n",
    "Keras can separate a portion of the training data into a validation dataset and evalute the performance of the model on that validation dataset each epoch. This can be enabled by setting the `validation_split` argument on the `fit()` function to a percentage of the size of your training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5effa70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "52/52 [==============================] - 1s 4ms/step - loss: 0.5381 - accuracy: 0.7179 - val_loss: 0.4770 - val_accuracy: 0.7795\n",
      "Epoch 2/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5247 - accuracy: 0.7276 - val_loss: 0.4828 - val_accuracy: 0.7874\n",
      "Epoch 3/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7276 - val_loss: 0.4912 - val_accuracy: 0.7638\n",
      "Epoch 4/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5282 - accuracy: 0.7393 - val_loss: 0.4951 - val_accuracy: 0.7402\n",
      "Epoch 5/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5397 - accuracy: 0.7198 - val_loss: 0.5066 - val_accuracy: 0.7795\n",
      "Epoch 6/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.7218 - val_loss: 0.4965 - val_accuracy: 0.7598\n",
      "Epoch 7/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5351 - accuracy: 0.7315 - val_loss: 0.4938 - val_accuracy: 0.7756\n",
      "Epoch 8/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5220 - accuracy: 0.7374 - val_loss: 0.4976 - val_accuracy: 0.7362\n",
      "Epoch 9/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5327 - accuracy: 0.7315 - val_loss: 0.5551 - val_accuracy: 0.7402\n",
      "Epoch 10/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5195 - accuracy: 0.7354 - val_loss: 0.5086 - val_accuracy: 0.7835\n",
      "Epoch 11/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5194 - accuracy: 0.7121 - val_loss: 0.4971 - val_accuracy: 0.7756\n",
      "Epoch 12/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5226 - accuracy: 0.7237 - val_loss: 0.5041 - val_accuracy: 0.7638\n",
      "Epoch 13/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5232 - accuracy: 0.7296 - val_loss: 0.5049 - val_accuracy: 0.7677\n",
      "Epoch 14/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5165 - accuracy: 0.7354 - val_loss: 0.5109 - val_accuracy: 0.7638\n",
      "Epoch 15/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5131 - accuracy: 0.7237 - val_loss: 0.4990 - val_accuracy: 0.7717\n",
      "Epoch 16/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5232 - accuracy: 0.7451 - val_loss: 0.4996 - val_accuracy: 0.7874\n",
      "Epoch 17/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5191 - accuracy: 0.7451 - val_loss: 0.5216 - val_accuracy: 0.7441\n",
      "Epoch 18/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5112 - accuracy: 0.7451 - val_loss: 0.5007 - val_accuracy: 0.7717\n",
      "Epoch 19/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5191 - accuracy: 0.7529 - val_loss: 0.4983 - val_accuracy: 0.7559\n",
      "Epoch 20/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5311 - accuracy: 0.7335 - val_loss: 0.5005 - val_accuracy: 0.7598\n",
      "Epoch 21/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5235 - accuracy: 0.7412 - val_loss: 0.5118 - val_accuracy: 0.7441\n",
      "Epoch 22/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5118 - accuracy: 0.7393 - val_loss: 0.4969 - val_accuracy: 0.7598\n",
      "Epoch 23/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5393 - accuracy: 0.7393 - val_loss: 0.4988 - val_accuracy: 0.7559\n",
      "Epoch 24/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5167 - accuracy: 0.7626 - val_loss: 0.5096 - val_accuracy: 0.7756\n",
      "Epoch 25/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5167 - accuracy: 0.7471 - val_loss: 0.5304 - val_accuracy: 0.7559\n",
      "Epoch 26/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5042 - accuracy: 0.7549 - val_loss: 0.5132 - val_accuracy: 0.7598\n",
      "Epoch 27/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5075 - accuracy: 0.7607 - val_loss: 0.5115 - val_accuracy: 0.7520\n",
      "Epoch 28/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5168 - accuracy: 0.7529 - val_loss: 0.4981 - val_accuracy: 0.7559\n",
      "Epoch 29/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5064 - accuracy: 0.7646 - val_loss: 0.5221 - val_accuracy: 0.7638\n",
      "Epoch 30/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5082 - accuracy: 0.7588 - val_loss: 0.5262 - val_accuracy: 0.7402\n",
      "Epoch 31/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7724 - val_loss: 0.5014 - val_accuracy: 0.7874\n",
      "Epoch 32/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5193 - accuracy: 0.7510 - val_loss: 0.5204 - val_accuracy: 0.7323\n",
      "Epoch 33/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7412 - val_loss: 0.5026 - val_accuracy: 0.7480\n",
      "Epoch 34/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5069 - accuracy: 0.7490 - val_loss: 0.4963 - val_accuracy: 0.7598\n",
      "Epoch 35/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5090 - accuracy: 0.7451 - val_loss: 0.5103 - val_accuracy: 0.7480\n",
      "Epoch 36/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4987 - accuracy: 0.7529 - val_loss: 0.5019 - val_accuracy: 0.7756\n",
      "Epoch 37/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5252 - accuracy: 0.7374 - val_loss: 0.5076 - val_accuracy: 0.7756\n",
      "Epoch 38/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4970 - accuracy: 0.7471 - val_loss: 0.5036 - val_accuracy: 0.7795\n",
      "Epoch 39/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5017 - accuracy: 0.7568 - val_loss: 0.5029 - val_accuracy: 0.7441\n",
      "Epoch 40/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4937 - accuracy: 0.7646 - val_loss: 0.5127 - val_accuracy: 0.7480\n",
      "Epoch 41/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5057 - accuracy: 0.7626 - val_loss: 0.5155 - val_accuracy: 0.7441\n",
      "Epoch 42/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5051 - accuracy: 0.7510 - val_loss: 0.5089 - val_accuracy: 0.7480\n",
      "Epoch 43/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7432 - val_loss: 0.5171 - val_accuracy: 0.7598\n",
      "Epoch 44/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5061 - accuracy: 0.7354 - val_loss: 0.5171 - val_accuracy: 0.7559\n",
      "Epoch 45/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5039 - accuracy: 0.7335 - val_loss: 0.5064 - val_accuracy: 0.7598\n",
      "Epoch 46/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5161 - accuracy: 0.7529 - val_loss: 0.5093 - val_accuracy: 0.7677\n",
      "Epoch 47/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5155 - accuracy: 0.7432 - val_loss: 0.5198 - val_accuracy: 0.7638\n",
      "Epoch 48/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5092 - accuracy: 0.7432 - val_loss: 0.5196 - val_accuracy: 0.7520\n",
      "Epoch 49/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5045 - accuracy: 0.7490 - val_loss: 0.5079 - val_accuracy: 0.7638\n",
      "Epoch 50/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4941 - accuracy: 0.7646 - val_loss: 0.5092 - val_accuracy: 0.7874\n",
      "Epoch 51/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5033 - accuracy: 0.7451 - val_loss: 0.5145 - val_accuracy: 0.7598\n",
      "Epoch 52/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4924 - accuracy: 0.7588 - val_loss: 0.5106 - val_accuracy: 0.7480\n",
      "Epoch 53/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4954 - accuracy: 0.7626 - val_loss: 0.5447 - val_accuracy: 0.7165\n",
      "Epoch 54/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4981 - accuracy: 0.7412 - val_loss: 0.5198 - val_accuracy: 0.7441\n",
      "Epoch 55/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5077 - accuracy: 0.7335 - val_loss: 0.5173 - val_accuracy: 0.7520\n",
      "Epoch 56/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5001 - accuracy: 0.7665 - val_loss: 0.5166 - val_accuracy: 0.7638\n",
      "Epoch 57/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5029 - accuracy: 0.7568 - val_loss: 0.5135 - val_accuracy: 0.7520\n",
      "Epoch 58/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4950 - accuracy: 0.7626 - val_loss: 0.5032 - val_accuracy: 0.7638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5067 - accuracy: 0.7471 - val_loss: 0.5124 - val_accuracy: 0.7559\n",
      "Epoch 60/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5105 - accuracy: 0.7296 - val_loss: 0.5143 - val_accuracy: 0.7677\n",
      "Epoch 61/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4966 - accuracy: 0.7646 - val_loss: 0.5306 - val_accuracy: 0.7402\n",
      "Epoch 62/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4962 - accuracy: 0.7860 - val_loss: 0.5369 - val_accuracy: 0.7283\n",
      "Epoch 63/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4908 - accuracy: 0.7665 - val_loss: 0.5376 - val_accuracy: 0.7559\n",
      "Epoch 64/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4977 - accuracy: 0.7724 - val_loss: 0.5406 - val_accuracy: 0.7244\n",
      "Epoch 65/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4997 - accuracy: 0.7626 - val_loss: 0.5136 - val_accuracy: 0.7795\n",
      "Epoch 66/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4932 - accuracy: 0.7626 - val_loss: 0.5023 - val_accuracy: 0.7638\n",
      "Epoch 67/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4993 - accuracy: 0.7510 - val_loss: 0.5066 - val_accuracy: 0.7638\n",
      "Epoch 68/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4948 - accuracy: 0.7549 - val_loss: 0.5251 - val_accuracy: 0.7559\n",
      "Epoch 69/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4992 - accuracy: 0.7607 - val_loss: 0.5076 - val_accuracy: 0.7598\n",
      "Epoch 70/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4943 - accuracy: 0.7646 - val_loss: 0.5499 - val_accuracy: 0.7165\n",
      "Epoch 71/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4912 - accuracy: 0.7665 - val_loss: 0.5218 - val_accuracy: 0.7598\n",
      "Epoch 72/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5128 - accuracy: 0.7724 - val_loss: 0.5129 - val_accuracy: 0.7756\n",
      "Epoch 73/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4943 - accuracy: 0.7724 - val_loss: 0.5237 - val_accuracy: 0.7559\n",
      "Epoch 74/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5073 - accuracy: 0.7471 - val_loss: 0.5530 - val_accuracy: 0.7323\n",
      "Epoch 75/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5042 - accuracy: 0.7588 - val_loss: 0.5246 - val_accuracy: 0.7283\n",
      "Epoch 76/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4963 - accuracy: 0.7646 - val_loss: 0.5240 - val_accuracy: 0.7480\n",
      "Epoch 77/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4882 - accuracy: 0.7685 - val_loss: 0.5576 - val_accuracy: 0.7402\n",
      "Epoch 78/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4976 - accuracy: 0.7685 - val_loss: 0.5361 - val_accuracy: 0.7441\n",
      "Epoch 79/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4918 - accuracy: 0.7588 - val_loss: 0.5472 - val_accuracy: 0.7362\n",
      "Epoch 80/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4949 - accuracy: 0.7704 - val_loss: 0.5559 - val_accuracy: 0.7402\n",
      "Epoch 81/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4939 - accuracy: 0.7626 - val_loss: 0.5063 - val_accuracy: 0.7520\n",
      "Epoch 82/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4917 - accuracy: 0.7743 - val_loss: 0.5025 - val_accuracy: 0.7598\n",
      "Epoch 83/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4937 - accuracy: 0.7529 - val_loss: 0.5399 - val_accuracy: 0.7480\n",
      "Epoch 84/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4878 - accuracy: 0.7802 - val_loss: 0.5297 - val_accuracy: 0.7520\n",
      "Epoch 85/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4899 - accuracy: 0.7724 - val_loss: 0.5158 - val_accuracy: 0.7480\n",
      "Epoch 86/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4818 - accuracy: 0.7899 - val_loss: 0.5335 - val_accuracy: 0.7598\n",
      "Epoch 87/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5032 - accuracy: 0.7568 - val_loss: 0.5216 - val_accuracy: 0.7638\n",
      "Epoch 88/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4860 - accuracy: 0.7685 - val_loss: 0.5254 - val_accuracy: 0.7559\n",
      "Epoch 89/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4877 - accuracy: 0.7763 - val_loss: 0.5192 - val_accuracy: 0.7717\n",
      "Epoch 90/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4846 - accuracy: 0.7840 - val_loss: 0.5322 - val_accuracy: 0.7362\n",
      "Epoch 91/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4931 - accuracy: 0.7724 - val_loss: 0.5323 - val_accuracy: 0.7520\n",
      "Epoch 92/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4801 - accuracy: 0.7879 - val_loss: 0.5477 - val_accuracy: 0.7559\n",
      "Epoch 93/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4807 - accuracy: 0.7724 - val_loss: 0.5552 - val_accuracy: 0.7362\n",
      "Epoch 94/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4775 - accuracy: 0.7840 - val_loss: 0.5212 - val_accuracy: 0.7598\n",
      "Epoch 95/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4910 - accuracy: 0.7646 - val_loss: 0.5683 - val_accuracy: 0.7283\n",
      "Epoch 96/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4886 - accuracy: 0.7763 - val_loss: 0.5236 - val_accuracy: 0.7677\n",
      "Epoch 97/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4868 - accuracy: 0.7665 - val_loss: 0.5208 - val_accuracy: 0.7520\n",
      "Epoch 98/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4911 - accuracy: 0.7626 - val_loss: 0.5428 - val_accuracy: 0.7402\n",
      "Epoch 99/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4831 - accuracy: 0.7802 - val_loss: 0.5493 - val_accuracy: 0.7598\n",
      "Epoch 100/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4867 - accuracy: 0.7704 - val_loss: 0.5685 - val_accuracy: 0.7323\n",
      "Epoch 101/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4840 - accuracy: 0.7763 - val_loss: 0.5263 - val_accuracy: 0.7441\n",
      "Epoch 102/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4847 - accuracy: 0.7840 - val_loss: 0.5269 - val_accuracy: 0.7559\n",
      "Epoch 103/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4792 - accuracy: 0.7685 - val_loss: 0.5527 - val_accuracy: 0.7480\n",
      "Epoch 104/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4871 - accuracy: 0.7704 - val_loss: 0.5441 - val_accuracy: 0.7520\n",
      "Epoch 105/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4736 - accuracy: 0.7821 - val_loss: 0.5224 - val_accuracy: 0.7559\n",
      "Epoch 106/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4733 - accuracy: 0.7879 - val_loss: 0.5370 - val_accuracy: 0.7520\n",
      "Epoch 107/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4767 - accuracy: 0.7743 - val_loss: 0.5523 - val_accuracy: 0.7480\n",
      "Epoch 108/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.7763 - val_loss: 0.5251 - val_accuracy: 0.7559\n",
      "Epoch 109/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4778 - accuracy: 0.7685 - val_loss: 0.5293 - val_accuracy: 0.7520\n",
      "Epoch 110/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4772 - accuracy: 0.7763 - val_loss: 0.5609 - val_accuracy: 0.7402\n",
      "Epoch 111/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4762 - accuracy: 0.7743 - val_loss: 0.5497 - val_accuracy: 0.7559\n",
      "Epoch 112/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4706 - accuracy: 0.7665 - val_loss: 0.5731 - val_accuracy: 0.7520\n",
      "Epoch 113/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4802 - accuracy: 0.7821 - val_loss: 0.5387 - val_accuracy: 0.7441\n",
      "Epoch 114/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4818 - accuracy: 0.7704 - val_loss: 0.5503 - val_accuracy: 0.7559\n",
      "Epoch 115/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4808 - accuracy: 0.7763 - val_loss: 0.5429 - val_accuracy: 0.7638\n",
      "Epoch 116/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4723 - accuracy: 0.7821 - val_loss: 0.5398 - val_accuracy: 0.7559\n",
      "Epoch 117/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4826 - accuracy: 0.7549 - val_loss: 0.5290 - val_accuracy: 0.7520\n",
      "Epoch 118/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4843 - accuracy: 0.7802 - val_loss: 0.5688 - val_accuracy: 0.7283\n",
      "Epoch 119/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5254 - accuracy: 0.7354 - val_loss: 0.5387 - val_accuracy: 0.7441\n",
      "Epoch 120/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4906 - accuracy: 0.7743 - val_loss: 0.5326 - val_accuracy: 0.7559\n",
      "Epoch 121/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4758 - accuracy: 0.7685 - val_loss: 0.5405 - val_accuracy: 0.7323\n",
      "Epoch 122/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4776 - accuracy: 0.7860 - val_loss: 0.5435 - val_accuracy: 0.7402\n",
      "Epoch 123/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4737 - accuracy: 0.7802 - val_loss: 0.5612 - val_accuracy: 0.7244\n",
      "Epoch 124/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4888 - accuracy: 0.7743 - val_loss: 0.5526 - val_accuracy: 0.7402\n",
      "Epoch 125/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4921 - accuracy: 0.7704 - val_loss: 0.5367 - val_accuracy: 0.7441\n",
      "Epoch 126/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4782 - accuracy: 0.7665 - val_loss: 0.5325 - val_accuracy: 0.7520\n",
      "Epoch 127/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4770 - accuracy: 0.7782 - val_loss: 0.5330 - val_accuracy: 0.7520\n",
      "Epoch 128/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4711 - accuracy: 0.7704 - val_loss: 0.5568 - val_accuracy: 0.7402\n",
      "Epoch 129/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4797 - accuracy: 0.7782 - val_loss: 0.5545 - val_accuracy: 0.7402\n",
      "Epoch 130/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4672 - accuracy: 0.7860 - val_loss: 0.5457 - val_accuracy: 0.7717\n",
      "Epoch 131/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4685 - accuracy: 0.7840 - val_loss: 0.5461 - val_accuracy: 0.7441\n",
      "Epoch 132/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4697 - accuracy: 0.7918 - val_loss: 0.5391 - val_accuracy: 0.7520\n",
      "Epoch 133/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4705 - accuracy: 0.7802 - val_loss: 0.5284 - val_accuracy: 0.7598\n",
      "Epoch 134/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4754 - accuracy: 0.7743 - val_loss: 0.5543 - val_accuracy: 0.7402\n",
      "Epoch 135/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4698 - accuracy: 0.7665 - val_loss: 0.5252 - val_accuracy: 0.7835\n",
      "Epoch 136/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4757 - accuracy: 0.7802 - val_loss: 0.5474 - val_accuracy: 0.7362\n",
      "Epoch 137/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4783 - accuracy: 0.7763 - val_loss: 0.5507 - val_accuracy: 0.7598\n",
      "Epoch 138/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4787 - accuracy: 0.7782 - val_loss: 0.5390 - val_accuracy: 0.7795\n",
      "Epoch 139/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4748 - accuracy: 0.7802 - val_loss: 0.5440 - val_accuracy: 0.7598\n",
      "Epoch 140/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5042 - accuracy: 0.7704 - val_loss: 0.5567 - val_accuracy: 0.7677\n",
      "Epoch 141/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4740 - accuracy: 0.7840 - val_loss: 0.5645 - val_accuracy: 0.7402\n",
      "Epoch 142/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4678 - accuracy: 0.7724 - val_loss: 0.5326 - val_accuracy: 0.7756\n",
      "Epoch 143/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4699 - accuracy: 0.7860 - val_loss: 0.5370 - val_accuracy: 0.7638\n",
      "Epoch 144/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4868 - accuracy: 0.7490 - val_loss: 0.5381 - val_accuracy: 0.7638\n",
      "Epoch 145/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4739 - accuracy: 0.7840 - val_loss: 0.5734 - val_accuracy: 0.7441\n",
      "Epoch 146/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4685 - accuracy: 0.7763 - val_loss: 0.5484 - val_accuracy: 0.7480\n",
      "Epoch 147/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4805 - accuracy: 0.7685 - val_loss: 0.5458 - val_accuracy: 0.7795\n",
      "Epoch 148/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4660 - accuracy: 0.7821 - val_loss: 0.5768 - val_accuracy: 0.7598\n",
      "Epoch 149/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4746 - accuracy: 0.7821 - val_loss: 0.5386 - val_accuracy: 0.7717\n",
      "Epoch 150/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4703 - accuracy: 0.7840 - val_loss: 0.5513 - val_accuracy: 0.7598\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe65da4f190>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, validation_split=0.33, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79458d4",
   "metadata": {},
   "source": [
    "### Using Manual Verification\n",
    "\n",
    "We can use the `train_test_split()` function from scikit-learn to separate our data into a training and validation dataset. The validation dataset can be specified to the `fit()` function in Keras by the `validation_data` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ed5a923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5103 - accuracy: 0.7704 - val_loss: 0.4978 - val_accuracy: 0.7717\n",
      "Epoch 2/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4818 - accuracy: 0.7782 - val_loss: 0.4907 - val_accuracy: 0.7677\n",
      "Epoch 3/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4930 - accuracy: 0.7646 - val_loss: 0.4828 - val_accuracy: 0.7795\n",
      "Epoch 4/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4894 - accuracy: 0.7665 - val_loss: 0.4876 - val_accuracy: 0.7953\n",
      "Epoch 5/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4909 - accuracy: 0.7588 - val_loss: 0.4898 - val_accuracy: 0.7874\n",
      "Epoch 6/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4969 - accuracy: 0.7568 - val_loss: 0.4832 - val_accuracy: 0.7717\n",
      "Epoch 7/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4795 - accuracy: 0.7724 - val_loss: 0.4957 - val_accuracy: 0.7638\n",
      "Epoch 8/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4793 - accuracy: 0.7685 - val_loss: 0.4853 - val_accuracy: 0.7835\n",
      "Epoch 9/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4747 - accuracy: 0.7860 - val_loss: 0.4880 - val_accuracy: 0.7677\n",
      "Epoch 10/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4747 - accuracy: 0.7685 - val_loss: 0.4922 - val_accuracy: 0.7756\n",
      "Epoch 11/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4764 - accuracy: 0.7646 - val_loss: 0.4958 - val_accuracy: 0.7677\n",
      "Epoch 12/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4814 - accuracy: 0.7510 - val_loss: 0.4895 - val_accuracy: 0.7717\n",
      "Epoch 13/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4686 - accuracy: 0.7704 - val_loss: 0.4920 - val_accuracy: 0.7756\n",
      "Epoch 14/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4847 - accuracy: 0.7802 - val_loss: 0.4953 - val_accuracy: 0.7756\n",
      "Epoch 15/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4838 - accuracy: 0.7607 - val_loss: 0.4930 - val_accuracy: 0.7717\n",
      "Epoch 16/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4767 - accuracy: 0.7646 - val_loss: 0.4941 - val_accuracy: 0.7795\n",
      "Epoch 17/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4732 - accuracy: 0.7685 - val_loss: 0.4854 - val_accuracy: 0.7717\n",
      "Epoch 18/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4717 - accuracy: 0.7763 - val_loss: 0.4885 - val_accuracy: 0.7717\n",
      "Epoch 19/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4753 - accuracy: 0.7626 - val_loss: 0.5109 - val_accuracy: 0.7756\n",
      "Epoch 20/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4738 - accuracy: 0.7802 - val_loss: 0.5115 - val_accuracy: 0.7480\n",
      "Epoch 21/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4742 - accuracy: 0.7685 - val_loss: 0.5109 - val_accuracy: 0.7638\n",
      "Epoch 22/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4607 - accuracy: 0.7743 - val_loss: 0.5089 - val_accuracy: 0.7717\n",
      "Epoch 23/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4726 - accuracy: 0.7879 - val_loss: 0.5030 - val_accuracy: 0.7717\n",
      "Epoch 24/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4633 - accuracy: 0.7840 - val_loss: 0.5354 - val_accuracy: 0.7087\n",
      "Epoch 25/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4658 - accuracy: 0.7879 - val_loss: 0.4943 - val_accuracy: 0.7756\n",
      "Epoch 26/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4653 - accuracy: 0.7704 - val_loss: 0.4864 - val_accuracy: 0.7756\n",
      "Epoch 27/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4593 - accuracy: 0.7860 - val_loss: 0.5063 - val_accuracy: 0.7638\n",
      "Epoch 28/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4639 - accuracy: 0.7763 - val_loss: 0.5038 - val_accuracy: 0.7756\n",
      "Epoch 29/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4639 - accuracy: 0.7860 - val_loss: 0.5204 - val_accuracy: 0.7638\n",
      "Epoch 30/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4649 - accuracy: 0.7821 - val_loss: 0.4994 - val_accuracy: 0.7598\n",
      "Epoch 31/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4615 - accuracy: 0.7646 - val_loss: 0.5132 - val_accuracy: 0.7520\n",
      "Epoch 32/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4718 - accuracy: 0.7743 - val_loss: 0.4970 - val_accuracy: 0.7756\n",
      "Epoch 33/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4629 - accuracy: 0.7840 - val_loss: 0.5273 - val_accuracy: 0.7283\n",
      "Epoch 34/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4609 - accuracy: 0.7860 - val_loss: 0.5082 - val_accuracy: 0.7677\n",
      "Epoch 35/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4575 - accuracy: 0.7821 - val_loss: 0.4853 - val_accuracy: 0.7756\n",
      "Epoch 36/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4587 - accuracy: 0.7879 - val_loss: 0.5070 - val_accuracy: 0.7598\n",
      "Epoch 37/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4532 - accuracy: 0.7879 - val_loss: 0.5068 - val_accuracy: 0.7677\n",
      "Epoch 38/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4610 - accuracy: 0.7646 - val_loss: 0.5015 - val_accuracy: 0.7835\n",
      "Epoch 39/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4576 - accuracy: 0.7879 - val_loss: 0.5032 - val_accuracy: 0.7638\n",
      "Epoch 40/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4576 - accuracy: 0.7879 - val_loss: 0.5135 - val_accuracy: 0.7480\n",
      "Epoch 41/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4550 - accuracy: 0.7996 - val_loss: 0.4881 - val_accuracy: 0.7756\n",
      "Epoch 42/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4604 - accuracy: 0.7685 - val_loss: 0.5037 - val_accuracy: 0.7717\n",
      "Epoch 43/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4552 - accuracy: 0.7899 - val_loss: 0.5202 - val_accuracy: 0.7638\n",
      "Epoch 44/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4539 - accuracy: 0.7724 - val_loss: 0.5324 - val_accuracy: 0.7205\n",
      "Epoch 45/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4517 - accuracy: 0.7899 - val_loss: 0.4816 - val_accuracy: 0.7835\n",
      "Epoch 46/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4590 - accuracy: 0.7879 - val_loss: 0.5345 - val_accuracy: 0.7441\n",
      "Epoch 47/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4496 - accuracy: 0.7782 - val_loss: 0.5215 - val_accuracy: 0.7717\n",
      "Epoch 48/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4532 - accuracy: 0.7899 - val_loss: 0.4905 - val_accuracy: 0.7835\n",
      "Epoch 49/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4489 - accuracy: 0.7860 - val_loss: 0.5022 - val_accuracy: 0.7598\n",
      "Epoch 50/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4412 - accuracy: 0.7957 - val_loss: 0.5276 - val_accuracy: 0.7362\n",
      "Epoch 51/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4609 - accuracy: 0.7685 - val_loss: 0.5091 - val_accuracy: 0.7480\n",
      "Epoch 52/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4472 - accuracy: 0.7977 - val_loss: 0.5243 - val_accuracy: 0.7323\n",
      "Epoch 53/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4540 - accuracy: 0.7879 - val_loss: 0.5207 - val_accuracy: 0.7283\n",
      "Epoch 54/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4558 - accuracy: 0.7840 - val_loss: 0.5307 - val_accuracy: 0.7205\n",
      "Epoch 55/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4556 - accuracy: 0.7724 - val_loss: 0.5886 - val_accuracy: 0.7087\n",
      "Epoch 56/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4904 - accuracy: 0.7626 - val_loss: 0.5383 - val_accuracy: 0.7126\n",
      "Epoch 57/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4471 - accuracy: 0.7840 - val_loss: 0.5096 - val_accuracy: 0.7520\n",
      "Epoch 58/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4477 - accuracy: 0.7899 - val_loss: 0.4868 - val_accuracy: 0.7638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4551 - accuracy: 0.7860 - val_loss: 0.5053 - val_accuracy: 0.7520\n",
      "Epoch 60/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4404 - accuracy: 0.7918 - val_loss: 0.4961 - val_accuracy: 0.7559\n",
      "Epoch 61/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4412 - accuracy: 0.7879 - val_loss: 0.4904 - val_accuracy: 0.7756\n",
      "Epoch 62/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4483 - accuracy: 0.7607 - val_loss: 0.5280 - val_accuracy: 0.7402\n",
      "Epoch 63/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4515 - accuracy: 0.7860 - val_loss: 0.5005 - val_accuracy: 0.7520\n",
      "Epoch 64/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4454 - accuracy: 0.7938 - val_loss: 0.4864 - val_accuracy: 0.7520\n",
      "Epoch 65/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4508 - accuracy: 0.7840 - val_loss: 0.5366 - val_accuracy: 0.7598\n",
      "Epoch 66/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4364 - accuracy: 0.7918 - val_loss: 0.4960 - val_accuracy: 0.7677\n",
      "Epoch 67/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4450 - accuracy: 0.7879 - val_loss: 0.5065 - val_accuracy: 0.7402\n",
      "Epoch 68/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4535 - accuracy: 0.7821 - val_loss: 0.5215 - val_accuracy: 0.7638\n",
      "Epoch 69/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4390 - accuracy: 0.7938 - val_loss: 0.5645 - val_accuracy: 0.6890\n",
      "Epoch 70/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4656 - accuracy: 0.7646 - val_loss: 0.5361 - val_accuracy: 0.7362\n",
      "Epoch 71/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4502 - accuracy: 0.7782 - val_loss: 0.4981 - val_accuracy: 0.7520\n",
      "Epoch 72/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4395 - accuracy: 0.7821 - val_loss: 0.5062 - val_accuracy: 0.7638\n",
      "Epoch 73/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4367 - accuracy: 0.7840 - val_loss: 0.5039 - val_accuracy: 0.7402\n",
      "Epoch 74/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4430 - accuracy: 0.7840 - val_loss: 0.4982 - val_accuracy: 0.7756\n",
      "Epoch 75/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4423 - accuracy: 0.7899 - val_loss: 0.5280 - val_accuracy: 0.7520\n",
      "Epoch 76/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4299 - accuracy: 0.7977 - val_loss: 0.4948 - val_accuracy: 0.7598\n",
      "Epoch 77/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4391 - accuracy: 0.7879 - val_loss: 0.4959 - val_accuracy: 0.7756\n",
      "Epoch 78/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4337 - accuracy: 0.7918 - val_loss: 0.5150 - val_accuracy: 0.7598\n",
      "Epoch 79/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4347 - accuracy: 0.7957 - val_loss: 0.5263 - val_accuracy: 0.7598\n",
      "Epoch 80/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4442 - accuracy: 0.7704 - val_loss: 0.5175 - val_accuracy: 0.7677\n",
      "Epoch 81/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4455 - accuracy: 0.7821 - val_loss: 0.4981 - val_accuracy: 0.7638\n",
      "Epoch 82/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4399 - accuracy: 0.7879 - val_loss: 0.5176 - val_accuracy: 0.7756\n",
      "Epoch 83/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4470 - accuracy: 0.7918 - val_loss: 0.5100 - val_accuracy: 0.7559\n",
      "Epoch 84/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4355 - accuracy: 0.8016 - val_loss: 0.5053 - val_accuracy: 0.7677\n",
      "Epoch 85/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4322 - accuracy: 0.7879 - val_loss: 0.5206 - val_accuracy: 0.7638\n",
      "Epoch 86/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4394 - accuracy: 0.7860 - val_loss: 0.5298 - val_accuracy: 0.7283\n",
      "Epoch 87/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4349 - accuracy: 0.7938 - val_loss: 0.5050 - val_accuracy: 0.7638\n",
      "Epoch 88/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4279 - accuracy: 0.8249 - val_loss: 0.5088 - val_accuracy: 0.7559\n",
      "Epoch 89/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4309 - accuracy: 0.7996 - val_loss: 0.5101 - val_accuracy: 0.7598\n",
      "Epoch 90/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4514 - accuracy: 0.7899 - val_loss: 0.4974 - val_accuracy: 0.7638\n",
      "Epoch 91/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4288 - accuracy: 0.8016 - val_loss: 0.5132 - val_accuracy: 0.7638\n",
      "Epoch 92/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4372 - accuracy: 0.7938 - val_loss: 0.4998 - val_accuracy: 0.7441\n",
      "Epoch 93/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4495 - accuracy: 0.7821 - val_loss: 0.5635 - val_accuracy: 0.6850\n",
      "Epoch 94/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4399 - accuracy: 0.7879 - val_loss: 0.4909 - val_accuracy: 0.7756\n",
      "Epoch 95/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4385 - accuracy: 0.7918 - val_loss: 0.5051 - val_accuracy: 0.7717\n",
      "Epoch 96/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4397 - accuracy: 0.7996 - val_loss: 0.5124 - val_accuracy: 0.7520\n",
      "Epoch 97/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4288 - accuracy: 0.7938 - val_loss: 0.5129 - val_accuracy: 0.7559\n",
      "Epoch 98/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4291 - accuracy: 0.7918 - val_loss: 0.5034 - val_accuracy: 0.7756\n",
      "Epoch 99/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4280 - accuracy: 0.8035 - val_loss: 0.5108 - val_accuracy: 0.7638\n",
      "Epoch 100/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4293 - accuracy: 0.7977 - val_loss: 0.5118 - val_accuracy: 0.7638\n",
      "Epoch 101/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4278 - accuracy: 0.8093 - val_loss: 0.5312 - val_accuracy: 0.7441\n",
      "Epoch 102/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4303 - accuracy: 0.8016 - val_loss: 0.4983 - val_accuracy: 0.7717\n",
      "Epoch 103/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4271 - accuracy: 0.7840 - val_loss: 0.5144 - val_accuracy: 0.7441\n",
      "Epoch 104/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4248 - accuracy: 0.7860 - val_loss: 0.4966 - val_accuracy: 0.7677\n",
      "Epoch 105/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4330 - accuracy: 0.7977 - val_loss: 0.5053 - val_accuracy: 0.7559\n",
      "Epoch 106/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4284 - accuracy: 0.7977 - val_loss: 0.5074 - val_accuracy: 0.7559\n",
      "Epoch 107/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4327 - accuracy: 0.7957 - val_loss: 0.5295 - val_accuracy: 0.7559\n",
      "Epoch 108/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4303 - accuracy: 0.8016 - val_loss: 0.5271 - val_accuracy: 0.7283\n",
      "Epoch 109/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4403 - accuracy: 0.7840 - val_loss: 0.5275 - val_accuracy: 0.7362\n",
      "Epoch 110/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4314 - accuracy: 0.7938 - val_loss: 0.4893 - val_accuracy: 0.7638\n",
      "Epoch 111/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4321 - accuracy: 0.7996 - val_loss: 0.5193 - val_accuracy: 0.7559\n",
      "Epoch 112/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4311 - accuracy: 0.8016 - val_loss: 0.5199 - val_accuracy: 0.7402\n",
      "Epoch 113/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4353 - accuracy: 0.8035 - val_loss: 0.5161 - val_accuracy: 0.7480\n",
      "Epoch 114/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4381 - accuracy: 0.7938 - val_loss: 0.5350 - val_accuracy: 0.7638\n",
      "Epoch 115/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4309 - accuracy: 0.8035 - val_loss: 0.5303 - val_accuracy: 0.7520\n",
      "Epoch 116/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4288 - accuracy: 0.7782 - val_loss: 0.6121 - val_accuracy: 0.7323\n",
      "Epoch 117/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4380 - accuracy: 0.7821 - val_loss: 0.5349 - val_accuracy: 0.7441\n",
      "Epoch 118/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4363 - accuracy: 0.7957 - val_loss: 0.5026 - val_accuracy: 0.7559\n",
      "Epoch 119/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4245 - accuracy: 0.8113 - val_loss: 0.4974 - val_accuracy: 0.7598\n",
      "Epoch 120/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4295 - accuracy: 0.8035 - val_loss: 0.5220 - val_accuracy: 0.7598\n",
      "Epoch 121/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4230 - accuracy: 0.7977 - val_loss: 0.5185 - val_accuracy: 0.7559\n",
      "Epoch 122/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4212 - accuracy: 0.8074 - val_loss: 0.5093 - val_accuracy: 0.7441\n",
      "Epoch 123/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4308 - accuracy: 0.7977 - val_loss: 0.5336 - val_accuracy: 0.7598\n",
      "Epoch 124/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4228 - accuracy: 0.7821 - val_loss: 0.5235 - val_accuracy: 0.7441\n",
      "Epoch 125/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4362 - accuracy: 0.7938 - val_loss: 0.5402 - val_accuracy: 0.7126\n",
      "Epoch 126/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4291 - accuracy: 0.8132 - val_loss: 0.5061 - val_accuracy: 0.7677\n",
      "Epoch 127/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4200 - accuracy: 0.8191 - val_loss: 0.5191 - val_accuracy: 0.7520\n",
      "Epoch 128/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4205 - accuracy: 0.8035 - val_loss: 0.4975 - val_accuracy: 0.7638\n",
      "Epoch 129/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4263 - accuracy: 0.7996 - val_loss: 0.5192 - val_accuracy: 0.7283\n",
      "Epoch 130/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4422 - accuracy: 0.7840 - val_loss: 0.5118 - val_accuracy: 0.7441\n",
      "Epoch 131/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4220 - accuracy: 0.8054 - val_loss: 0.5135 - val_accuracy: 0.7559\n",
      "Epoch 132/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4207 - accuracy: 0.8093 - val_loss: 0.5198 - val_accuracy: 0.7362\n",
      "Epoch 133/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4239 - accuracy: 0.7879 - val_loss: 0.5386 - val_accuracy: 0.7638\n",
      "Epoch 134/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4286 - accuracy: 0.7996 - val_loss: 0.4961 - val_accuracy: 0.7638\n",
      "Epoch 135/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4229 - accuracy: 0.8171 - val_loss: 0.5515 - val_accuracy: 0.7598\n",
      "Epoch 136/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4226 - accuracy: 0.7996 - val_loss: 0.5553 - val_accuracy: 0.7165\n",
      "Epoch 137/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4379 - accuracy: 0.7763 - val_loss: 0.5000 - val_accuracy: 0.7677\n",
      "Epoch 138/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4155 - accuracy: 0.8074 - val_loss: 0.5065 - val_accuracy: 0.7559\n",
      "Epoch 139/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4145 - accuracy: 0.7879 - val_loss: 0.5358 - val_accuracy: 0.7677\n",
      "Epoch 140/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4244 - accuracy: 0.7938 - val_loss: 0.5328 - val_accuracy: 0.7362\n",
      "Epoch 141/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4195 - accuracy: 0.8054 - val_loss: 0.5113 - val_accuracy: 0.7638\n",
      "Epoch 142/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4200 - accuracy: 0.8074 - val_loss: 0.5144 - val_accuracy: 0.7480\n",
      "Epoch 143/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4294 - accuracy: 0.8035 - val_loss: 0.5052 - val_accuracy: 0.7520\n",
      "Epoch 144/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4208 - accuracy: 0.7918 - val_loss: 0.5272 - val_accuracy: 0.7638\n",
      "Epoch 145/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4155 - accuracy: 0.8074 - val_loss: 0.5496 - val_accuracy: 0.7402\n",
      "Epoch 146/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4251 - accuracy: 0.8035 - val_loss: 0.5154 - val_accuracy: 0.7677\n",
      "Epoch 147/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4331 - accuracy: 0.7977 - val_loss: 0.5226 - val_accuracy: 0.7441\n",
      "Epoch 148/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4301 - accuracy: 0.8093 - val_loss: 0.5036 - val_accuracy: 0.7717\n",
      "Epoch 149/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4285 - accuracy: 0.8054 - val_loss: 0.5042 - val_accuracy: 0.7480\n",
      "Epoch 150/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4129 - accuracy: 0.8152 - val_loss: 0.5135 - val_accuracy: 0.7677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe65e3d96a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d1167f",
   "metadata": {},
   "source": [
    "### Manual *k*-Fold Cross-Validation\n",
    "\n",
    "The gold standard for machine learning model evaluation in *k*-fold cross-validation. It provides a robust estimate of the performance of a model on unseen data. It does this by splitting the training dataset into *k* subsets and takes turns training models on all subsets except one which is held out, and evaluating model performance on the held out validation dataset. The process is repeated until all subsets are given an opportunity to be the held out validation set. The performance measure is then averaged across all models that are created. \n",
    "\n",
    "Cross-validation is often **not** used for evaluating deep learning models because of the greater computational expense. For example *k*-fold cross-validation is often used with 5 or 10 folds. As such, 5 or 10 models must be constructed and evaluated, greatly adding to the evaluation time of a model. Nevertheless, when the problem is small enough or if there are sufficient compute resources, *k*-fold cross-validation can give a less biased estimate of the peformance of the model. \n",
    "\n",
    "We can use the `StratifiedKFold` class from scikit-learn to split the training dataset into 10 folds. The folds are straified, meaning that the algorithm attempts to balance the number of instances of each class in each fold. The example creates and evaluates 10 models using the 10 splits of the data and collects all of the scores. The average and standard deviation of the model performance is then printed at the end of the run to provide a robust estimate of model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e4e6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebdca9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 75.32%\n",
      "accuracy: 83.12%\n",
      "accuracy: 79.22%\n",
      "accuracy: 83.12%\n",
      "accuracy: 74.03%\n",
      "accuracy: 75.32%\n",
      "accuracy: 79.22%\n",
      "accuracy: 67.53%\n",
      "accuracy: 78.95%\n",
      "accuracy: 81.58%\n",
      "77.74% (+/- 4.56%)\n"
     ]
    }
   ],
   "source": [
    "# define 10-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "cvscores = []\n",
    "\n",
    "# fitting models on each split (we will use the existing model already defined and compiled)\n",
    "for train, test in kfold.split(X, y):\n",
    "    # fit the model\n",
    "    model.fit(X[train], y[train], epochs=150, batch_size=10, verbose=0)\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b20ce",
   "metadata": {},
   "source": [
    "## Leveraging Scikit-Learn\n",
    "\n",
    "The scikit-learn library is the most popular library for general machine learning in Python. We can use scikit-learn as a wrapper for Keras to employ the familiar functions and methods used in machine learning development. The Keras library provides a convenient wrapper for deep learning models to be used as classification or regression estimators in scikit-learn. We will work through examples of using the `KerasClassifier` wrapper for a classification neural network created in Keras and used in the scikit-learn library. Relatively recently, the keras.wrappers.scikit_learn library has been deprecated in favor of the scikeras library. We will use the latter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5cefaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install scikeras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3340b11",
   "metadata": {},
   "source": [
    "The `KerasClassifier` and `KerasRegressor` classes in Keras take an argument `build_fn` which is the name of the function to call to create the model. We must define a function that defines the model, compiles it, and returns it. In the example to follow, we will define a function called `create_model()` which will perform these steps for us.\n",
    "\n",
    "We can then pass this function name to the `KerasClassifier` class by the `build_fn` argument. We also pass in additional arguments of `epochs=150` and `batch_size=10`. These are automatically bundled up and passed on to the `fit()` function which is called internally by the `KerasClassifier` class. We then can use the scikit-learn function `cross_val_score()` to evaluate our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de968ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b21ae6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb5e6b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model using KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11ff24b",
   "metadata": {},
   "source": [
    "> Note: In a future relase, the `build_fn` argument will be named `model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af3fb10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7331168831168832\n"
     ]
    }
   ],
   "source": [
    "# Evaluate using 10-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(model, X, y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa49332",
   "metadata": {},
   "source": [
    "When the Keras model is wrapped that estimating model accuracy can be greatly streamlined, compared to the manual enumeration of cross-validation folds.\n",
    "\n",
    "## Grid Search of Deep Learning Model Parameters\n",
    "\n",
    "We already know we can provide arguments to the `fit()` function. The function that we specify to the `build_fn` argument when creating the `KerasClassifier` wrapper can also take arguments. We can use these arguments to further customize the construction of the model. \n",
    "\n",
    "In this example, we use a grid search to evaluate different configurations for our neural network model and report on the combination that provides the best estimated performance. The `create_model()` function is defined to take two arguments `optimizer` and `init`, both of which must have default values. This will allow us to evaluate the effect of using different optimization algorithms and weight initialization schemes for our network. After creating our model, we define arrays of values for the parameter we wish to search, specifically:\n",
    "- Optimizers for searching different weight values.\n",
    "- Initializers for preparing the network weights using different schemes.\n",
    "- Number of eopchs for training the model for different number of exposures to the training dataset.\n",
    "- Batches for varying th enumber of samples before weight updates.\n",
    "\n",
    "The options are specified into a dictionary and passed to the configuration of the `GridSearchCV` scikit-learn class. This class will evaluate a version of our nerual network model for each combination of parameters (2 x 3 x 3 x 3) for the combinations of optimizers, initializations, epochs and batches). Each combination is then evaluated using the default of 3-fold stratified cross-validation. \n",
    "\n",
    "> Note: It may be useful to design small experiments with a smaller subset of the data, as this process can be incredibly compute and time intensive. \n",
    "\n",
    "Finally, the performance and combination of configurations for the best model are displayed, followed by the performance of all combinations of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea217b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c604547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier with optimizer and init argument values passed\n",
    "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(8, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ea7abbc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter init for estimator KerasClassifier.\nThis issue can likely be resolved by setting this parameter in the KerasClassifier constructor:\n`KerasClassifier(init=glorot_uniform)`\nCheck the list of available parameters with `estimator.get_params().keys()`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-9cb1920c0bf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0mcloned_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcloned_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m   1166\u001b[0m                     \u001b[0;31m# Give a SciKeras specific user message to aid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m                     \u001b[0;31m# in moving from the Keras wrappers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m   1169\u001b[0m                         \u001b[0;34mf\"Invalid parameter {param} for estimator {self.__name__}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m                         \u001b[0;34m\"\\nThis issue can likely be resolved by setting this parameter\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter init for estimator KerasClassifier.\nThis issue can likely be resolved by setting this parameter in the KerasClassifier constructor:\n`KerasClassifier(init=glorot_uniform)`\nCheck the list of available parameters with `estimator.get_params().keys()`"
     ]
    }
   ],
   "source": [
    "# Create model with KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Grid search epocs, batch size, and optimizer\n",
    "\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "epochs = [50, 100, 150]\n",
    "batches = [5, 10, 20]\n",
    "inits = ['glorot_uniform', 'normal', 'uniform']\n",
    "\n",
    "param_grid = dict(optimizers=optimizers, epochs=epochs, batch_size=batches, init=inits)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438ee89b",
   "metadata": {},
   "source": [
    "Note: Recent deprectation of the keras.wrappers.scikit_learn library and the subsequent shift to the scikeras library has caused issues with the typical coding of grid search parameters. This error will be resolved in this notebook and the investigation will continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4d847b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
