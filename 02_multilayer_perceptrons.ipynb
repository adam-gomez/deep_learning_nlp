{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa05ed2",
   "metadata": {},
   "source": [
    "## A Brief Introduction to Multilayer Perceptions\n",
    "A Perceptron is a single neuron model that was a precursor to larger neural networks. A perceptron is able to receive an single dimensional array of inputs and produce an single dimensional array of outputs that can be used for a variety of ML tasks, the most fundamental of which is classification. \n",
    "\n",
    "The building block for neural networks are artificial neurons. These are simple computational units that have weighted input signals and produce an output signal using an activation function. \n",
    "\n",
    "The weights attached to input signals can be likened to the coefficients of a regression equation. These weights represent the level of importance for each input. Consequently, changes in some inputs will have a larger effect on the output of the perceptron than others. Furthermore, each neuron will have a bias, a scalar weight that adjusts the final output of the neuron by a set amount, positive or negative.\n",
    "\n",
    "Weights are often initialized to small random values although more complex initialization schemes can be used. Larger weights tend to increase the complexity and fragility of the model, so it is desirable to keep weights small and regularization techniques can be used to decrease the risks of overfitting. \n",
    "\n",
    "### Activation\n",
    "\n",
    "The weighted inputs are summed and passed through an **activation function**, sometimes called a **transfer function**. It is called an activation function because it governs the threshold at which the neuron is \"activated\" and the strength of the output signal. The output of the function is limited in various ways, from application of the sigmoid function to bind values between 0 and 1, to hyperbolic tangent functions (Tanh) that binds values between -1 and 1, to ReLU (rectifier activation function) that binds values between 0 and another positive number. ReLU has been found to provide better results, generally, and this may be due to its mimicry of all-or-nothing type activation systems found in biological neurology. \n",
    "\n",
    "### Network of Neurons\n",
    "\n",
    "Neurons are arranged into networks of neurons. A row of neurons is called a **layer** and one network can have multiple layers. The architecture of the neurons in the network is often called the **network topology**. \n",
    "\n",
    "### Input of Visible Layers\n",
    "\n",
    "The bottom layer that takes input from a dataset is called the visible layer, because it is the exposed part of the network. Fundamentally, it is simply the series of initial inputs, where each input is the value from each column in the dataset for a given observation. \n",
    "\n",
    "### Hidden Layers\n",
    "Layers after the input layer are called **hidden layers** because they are not directly exposed to the input. Each input is effectively transformed through a weight and bias. \n",
    "\n",
    "### Output Layer\n",
    "\n",
    "The final hidden layer is called the output layer and is responsible for producing a value or vector of values that correspond to the format required for the problem. Here are some examples:\n",
    "- A regression problem may have a single output neuron and the neuron may have no activation function.\n",
    "- A binary classification problem may have a single output neuron and use a sigmoid activation function to output a value between 0 and 1 to represent the probability of predicting a value for the primary class. This can be turned into a crisp class value by using a threshold of 0.5 and snap values less than the threshold to 0 otherwise to 1. \n",
    "- A multiclass classification problem may have multiple neurons in the output layer, one for each class (e.g. three neurons for the three classes in the famous iris flowers classification problem). In this case, a softmax activation function may be used to output a probability of the network predicting each of the class values. Selecting the output with the highest probability can be used to produce a crisp class classification value. \n",
    "\n",
    "## Training Networks\n",
    "Once configured, the neural network must be trained on the dataset.\n",
    "\n",
    "### Data Preparation\n",
    "- Data must be numerical\n",
    "    - Categorical data can be converted via *one-hot encoding*.\n",
    "    - Text data will need to be converted via TF-IDF or some other NLP encoding technique.\n",
    "    - This same one hot encoding can be used on the output variable in classification problems with more than one class.\n",
    "- Data must be similarly scaled\n",
    "    - Min-max scaling (or any other scaling transformation) can be applied to the dataset to ensure consistency between inputs\n",
    "    \n",
    "### Stochastic Gradient Descent\n",
    "The classical training algorithm for neural networks is stochastic gradient descent. A single observation is exposed to the network, and the output is generated (called a **forward pass**). The output is compared to the expected output and an error is calculated. The error is propagated back through the network, one layer at a time, and the weights are updated according to the amount they contributed ot the error (known as **backpropagation**). One round of updating the network for the entire training dataset is called an **epoch**. A network may be trained for any number of epochs. \n",
    "\n",
    "### Weight Updates\n",
    "The weights can be updated from the errors calculated after each forward pass for each observation. This is called **online learning**. Alternatively, the errors can be aggregated across all results of all observations in the training set. This is called **batch learning** and is often more stable. \n",
    "\n",
    "Because datasets are so large, the size of the batch will be reduced before an update is completed. The amount that the weights are updated is controlled by a configuration parameter called the learning rate. This is also called the **step size** and controls the momentum at which a local minima is approached in order to reduce the risk of a convergence failure (where a local minima cannot be identified because each update overshoots a true local minima of the aggregated errors). \n",
    "- **Momentum** is a term that incorporates the properties from the previous weight update to allow the weights to continue to change in the same direction even when there is less error being calculated.\n",
    "- **Learning Rate Decay** is used to decrease the learning weight over epochs to allow the network to make large changes to the weights at the beginning and smaller fine tuning changes later in the training schedule.\n",
    "\n",
    "### Prediction\n",
    "Predictions are made by providing the input to the network and performing a forward-pass allowing it to generate an ouput that you can use as a prediction. Evaluating performance on a separate out-of-sample dataset (ideally randomly split from the original full dataset available) can alert to risks of overfitting.\n",
    "\n",
    "The network topology and the final set of weights is all that needs to persist from the research environment into the production environment to make novel predictions. \n",
    "\n",
    "## Developing a Simple Neural Network from the Pima Indians Onset of Diabetes Dataset\n",
    "\n",
    "This is a standard machine learning dataset available from the UCI Machine Learning repository. It describes patient medical record data for Pima Indians and whether they had an onset of diabetes within five years. It is a binary classification problem (onset of diabetes as 1 or not as 0). Below are the attributes for the dataset:\n",
    "1. Number of times pregnant.\n",
    "2. Plasma glucose concentration at 2 hours in an oral glucose tolerance test.\n",
    "3. Diastolic blood pressure (mm Hg).\n",
    "4. Triceps skin fold thickness (mm).\n",
    "5. 2-Hour serum insulin (mu U/ml).\n",
    "6. Body mass index.\n",
    "7. Diabetes pedigree function.\n",
    "8. Age (years).\n",
    "9. Class, onset of diabetes within five years.\n",
    "\n",
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf72d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6da56488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.   , 148.   ,  72.   , ...,   0.627,  50.   ,   1.   ],\n",
       "       [  1.   ,  85.   ,  66.   , ...,   0.351,  31.   ,   0.   ],\n",
       "       [  8.   , 183.   ,  64.   , ...,   0.672,  32.   ,   1.   ],\n",
       "       ...,\n",
       "       [  5.   , 121.   ,  72.   , ...,   0.245,  30.   ,   0.   ],\n",
       "       [  1.   , 126.   ,  60.   , ...,   0.349,  47.   ,   1.   ],\n",
       "       [  1.   ,  93.   ,  70.   , ...,   0.315,  23.   ,   0.   ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the dataset\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af22c7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input (X) and output (y) variables\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d18b3a3",
   "metadata": {},
   "source": [
    "## Defining the Model\n",
    "\n",
    "Models in Keras are a sequence of layers. We create a `Sequential` model and add layers one at a time until we are happy with our network topology. The first thing to get right is to ensure the input layer has the right number of inputs. This can be specified when creating the first layer with the `input_dim` argument and setting it to `8` for the 8 input variables. \n",
    "\n",
    "How do we know the number of layers to use and their types? Trial and error experimentation. Generally, you need a network large enough to capture the structure of the problem. In this example, we will use a fully-connected network structure with three layers.\n",
    "\n",
    "Fully connected layers are defined using the `Dense` class. We can specify the number f neurons in the layer as the first argument and specify the activation function using the `activation` argument. We will use the rectifier (`relu`) activation function on the first two layers and the `sigmoid` activation function in the output layer. It used to be the case that sigmoid and `tanh` activation functions were preferred for all layers. These days, better performance is seen using the rectifier activation function. For the output layer, we can snap the classification by setting a threshold, in this case 0.5.\n",
    "\n",
    "The first hidden layer will have 12 neurons and expect 8 input variables (e.g. `input_dim=8`). The second hidden layer will have 8 neurons and finally the output layer will have 1 neuron to predict the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24473260",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3988b269",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "\n",
    "The backend will automatically choose the best way to represent the network for training and making predictions to run on my hardware. When compiling, we must specify some additional properties required when training the network:\n",
    "- The loss function to used to evaluate a set of weights\n",
    "- The optimizer used to serach through different weights \n",
    "- Any optional metrics we would like to collect and report\n",
    "\n",
    "In this case, we will use logarithmic loss, which for a binary classification problem is defined in Keras as `binary_crossentropy`. We will use the gradient descent algorithm `adam` as it is an efficient default. For metrics, we will capture and report the model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fcf4839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d83fe",
   "metadata": {},
   "source": [
    "## Fit the Model\n",
    "\n",
    "We have defined our model and compiled it for efficient computation. Now it is time to execute the model on some data. We can train on our loaded dat by calling the `fit()` function on the model.\n",
    "\n",
    "The training process will run for a fixed number of iterations through the dataset called epochs, that we must specify with the `epochs` argument. We can also set the number of instances that are evaluated before a weight update in the network is performed called the batch sixe and set using the `batch_size` argument. For this problem we will run a small number of epochs (150) and use a relatively small batch size of 16. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ea6408e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "48/48 [==============================] - 1s 949us/step - loss: 4.0663 - accuracy: 0.5234\n",
      "Epoch 2/150\n",
      "48/48 [==============================] - 0s 866us/step - loss: 1.9515 - accuracy: 0.6185\n",
      "Epoch 3/150\n",
      "48/48 [==============================] - 0s 847us/step - loss: 1.6644 - accuracy: 0.6146\n",
      "Epoch 4/150\n",
      "48/48 [==============================] - 0s 877us/step - loss: 1.5068 - accuracy: 0.6146\n",
      "Epoch 5/150\n",
      "48/48 [==============================] - 0s 866us/step - loss: 1.2874 - accuracy: 0.6107\n",
      "Epoch 6/150\n",
      "48/48 [==============================] - 0s 880us/step - loss: 1.1270 - accuracy: 0.6393\n",
      "Epoch 7/150\n",
      "48/48 [==============================] - 0s 883us/step - loss: 1.0279 - accuracy: 0.6367\n",
      "Epoch 8/150\n",
      "48/48 [==============================] - 0s 903us/step - loss: 0.9399 - accuracy: 0.6250\n",
      "Epoch 9/150\n",
      "48/48 [==============================] - 0s 912us/step - loss: 0.8609 - accuracy: 0.6328\n",
      "Epoch 10/150\n",
      "48/48 [==============================] - 0s 881us/step - loss: 0.8120 - accuracy: 0.6484\n",
      "Epoch 11/150\n",
      "48/48 [==============================] - 0s 761us/step - loss: 0.7677 - accuracy: 0.6523\n",
      "Epoch 12/150\n",
      "48/48 [==============================] - 0s 735us/step - loss: 0.7523 - accuracy: 0.6458\n",
      "Epoch 13/150\n",
      "48/48 [==============================] - 0s 730us/step - loss: 0.7664 - accuracy: 0.6667\n",
      "Epoch 14/150\n",
      "48/48 [==============================] - 0s 775us/step - loss: 0.6880 - accuracy: 0.6758\n",
      "Epoch 15/150\n",
      "48/48 [==============================] - 0s 764us/step - loss: 0.6711 - accuracy: 0.6654\n",
      "Epoch 16/150\n",
      "48/48 [==============================] - 0s 738us/step - loss: 0.6620 - accuracy: 0.6797\n",
      "Epoch 17/150\n",
      "48/48 [==============================] - 0s 732us/step - loss: 0.6422 - accuracy: 0.6875\n",
      "Epoch 18/150\n",
      "48/48 [==============================] - 0s 738us/step - loss: 0.6460 - accuracy: 0.6745\n",
      "Epoch 19/150\n",
      "48/48 [==============================] - 0s 711us/step - loss: 0.8064 - accuracy: 0.6354\n",
      "Epoch 20/150\n",
      "48/48 [==============================] - 0s 761us/step - loss: 0.6750 - accuracy: 0.6901\n",
      "Epoch 21/150\n",
      "48/48 [==============================] - 0s 727us/step - loss: 0.6324 - accuracy: 0.6940\n",
      "Epoch 22/150\n",
      "48/48 [==============================] - 0s 852us/step - loss: 0.6364 - accuracy: 0.6888\n",
      "Epoch 23/150\n",
      "48/48 [==============================] - 0s 892us/step - loss: 0.6500 - accuracy: 0.6862\n",
      "Epoch 24/150\n",
      "48/48 [==============================] - 0s 871us/step - loss: 0.6758 - accuracy: 0.6888\n",
      "Epoch 25/150\n",
      "48/48 [==============================] - 0s 868us/step - loss: 0.6277 - accuracy: 0.7018\n",
      "Epoch 26/150\n",
      "48/48 [==============================] - 0s 882us/step - loss: 0.5923 - accuracy: 0.7005\n",
      "Epoch 27/150\n",
      "48/48 [==============================] - 0s 844us/step - loss: 0.6300 - accuracy: 0.6875\n",
      "Epoch 28/150\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.6221 - accuracy: 0.7018\n",
      "Epoch 29/150\n",
      "48/48 [==============================] - 0s 716us/step - loss: 0.6247 - accuracy: 0.7018\n",
      "Epoch 30/150\n",
      "48/48 [==============================] - 0s 735us/step - loss: 0.6177 - accuracy: 0.6914\n",
      "Epoch 31/150\n",
      "48/48 [==============================] - 0s 832us/step - loss: 0.5795 - accuracy: 0.7018\n",
      "Epoch 32/150\n",
      "48/48 [==============================] - 0s 843us/step - loss: 0.6575 - accuracy: 0.6745\n",
      "Epoch 33/150\n",
      "48/48 [==============================] - 0s 800us/step - loss: 0.6029 - accuracy: 0.7096\n",
      "Epoch 34/150\n",
      "48/48 [==============================] - 0s 870us/step - loss: 0.5776 - accuracy: 0.7161\n",
      "Epoch 35/150\n",
      "48/48 [==============================] - 0s 851us/step - loss: 0.5815 - accuracy: 0.7057\n",
      "Epoch 36/150\n",
      "48/48 [==============================] - 0s 831us/step - loss: 0.5699 - accuracy: 0.7148\n",
      "Epoch 37/150\n",
      "48/48 [==============================] - 0s 844us/step - loss: 0.5831 - accuracy: 0.7214\n",
      "Epoch 38/150\n",
      "48/48 [==============================] - 0s 712us/step - loss: 0.5763 - accuracy: 0.7148\n",
      "Epoch 39/150\n",
      "48/48 [==============================] - 0s 734us/step - loss: 0.5680 - accuracy: 0.7188\n",
      "Epoch 40/150\n",
      "48/48 [==============================] - 0s 733us/step - loss: 0.5623 - accuracy: 0.7279\n",
      "Epoch 41/150\n",
      "48/48 [==============================] - 0s 771us/step - loss: 0.5544 - accuracy: 0.7279\n",
      "Epoch 42/150\n",
      "48/48 [==============================] - 0s 714us/step - loss: 0.5980 - accuracy: 0.7279\n",
      "Epoch 43/150\n",
      "48/48 [==============================] - 0s 736us/step - loss: 0.5849 - accuracy: 0.7057\n",
      "Epoch 44/150\n",
      "48/48 [==============================] - 0s 738us/step - loss: 0.5678 - accuracy: 0.7135\n",
      "Epoch 45/150\n",
      "48/48 [==============================] - 0s 724us/step - loss: 0.5661 - accuracy: 0.7357\n",
      "Epoch 46/150\n",
      "48/48 [==============================] - 0s 726us/step - loss: 0.5626 - accuracy: 0.7240\n",
      "Epoch 47/150\n",
      "48/48 [==============================] - 0s 707us/step - loss: 0.5582 - accuracy: 0.7331\n",
      "Epoch 48/150\n",
      "48/48 [==============================] - 0s 728us/step - loss: 0.5769 - accuracy: 0.7266\n",
      "Epoch 49/150\n",
      "48/48 [==============================] - 0s 732us/step - loss: 0.5675 - accuracy: 0.7148\n",
      "Epoch 50/150\n",
      "48/48 [==============================] - 0s 736us/step - loss: 0.6112 - accuracy: 0.6979\n",
      "Epoch 51/150\n",
      "48/48 [==============================] - 0s 727us/step - loss: 0.5864 - accuracy: 0.7122\n",
      "Epoch 52/150\n",
      "48/48 [==============================] - 0s 807us/step - loss: 0.6763 - accuracy: 0.7005\n",
      "Epoch 53/150\n",
      "48/48 [==============================] - 0s 763us/step - loss: 0.5748 - accuracy: 0.7253\n",
      "Epoch 54/150\n",
      "48/48 [==============================] - 0s 700us/step - loss: 0.6011 - accuracy: 0.7083\n",
      "Epoch 55/150\n",
      "48/48 [==============================] - 0s 704us/step - loss: 0.5503 - accuracy: 0.7370\n",
      "Epoch 56/150\n",
      "48/48 [==============================] - 0s 731us/step - loss: 0.5521 - accuracy: 0.7357\n",
      "Epoch 57/150\n",
      "48/48 [==============================] - 0s 829us/step - loss: 0.5562 - accuracy: 0.7461\n",
      "Epoch 58/150\n",
      "48/48 [==============================] - 0s 771us/step - loss: 0.5805 - accuracy: 0.7201\n",
      "Epoch 59/150\n",
      "48/48 [==============================] - 0s 824us/step - loss: 0.5877 - accuracy: 0.7253\n",
      "Epoch 60/150\n",
      "48/48 [==============================] - 0s 822us/step - loss: 0.5942 - accuracy: 0.7096\n",
      "Epoch 61/150\n",
      "48/48 [==============================] - 0s 708us/step - loss: 0.5693 - accuracy: 0.7161\n",
      "Epoch 62/150\n",
      "48/48 [==============================] - 0s 715us/step - loss: 0.5719 - accuracy: 0.7279\n",
      "Epoch 63/150\n",
      "48/48 [==============================] - 0s 709us/step - loss: 0.5514 - accuracy: 0.7174\n",
      "Epoch 64/150\n",
      "48/48 [==============================] - 0s 707us/step - loss: 0.5744 - accuracy: 0.7174\n",
      "Epoch 65/150\n",
      "48/48 [==============================] - 0s 713us/step - loss: 0.5513 - accuracy: 0.7513\n",
      "Epoch 66/150\n",
      "48/48 [==============================] - 0s 741us/step - loss: 0.5542 - accuracy: 0.7331\n",
      "Epoch 67/150\n",
      "48/48 [==============================] - 0s 729us/step - loss: 0.5532 - accuracy: 0.7409\n",
      "Epoch 68/150\n",
      "48/48 [==============================] - 0s 852us/step - loss: 0.5548 - accuracy: 0.7279\n",
      "Epoch 69/150\n",
      "48/48 [==============================] - 0s 832us/step - loss: 0.5606 - accuracy: 0.7227\n",
      "Epoch 70/150\n",
      "48/48 [==============================] - 0s 830us/step - loss: 0.5738 - accuracy: 0.7096\n",
      "Epoch 71/150\n",
      "48/48 [==============================] - 0s 777us/step - loss: 0.5616 - accuracy: 0.7422\n",
      "Epoch 72/150\n",
      "48/48 [==============================] - 0s 751us/step - loss: 0.5355 - accuracy: 0.7422\n",
      "Epoch 73/150\n",
      "48/48 [==============================] - 0s 783us/step - loss: 0.5956 - accuracy: 0.7070\n",
      "Epoch 74/150\n",
      "48/48 [==============================] - 0s 834us/step - loss: 0.5440 - accuracy: 0.7409\n",
      "Epoch 75/150\n",
      "48/48 [==============================] - 0s 862us/step - loss: 0.5386 - accuracy: 0.7422\n",
      "Epoch 76/150\n",
      "48/48 [==============================] - 0s 806us/step - loss: 0.5801 - accuracy: 0.7331\n",
      "Epoch 77/150\n",
      "48/48 [==============================] - 0s 728us/step - loss: 0.5360 - accuracy: 0.7331\n",
      "Epoch 78/150\n",
      "48/48 [==============================] - 0s 733us/step - loss: 0.5568 - accuracy: 0.7201\n",
      "Epoch 79/150\n",
      "48/48 [==============================] - 0s 792us/step - loss: 0.5584 - accuracy: 0.7188\n",
      "Epoch 80/150\n",
      "48/48 [==============================] - 0s 803us/step - loss: 0.5696 - accuracy: 0.7292\n",
      "Epoch 81/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 831us/step - loss: 0.5329 - accuracy: 0.7500\n",
      "Epoch 82/150\n",
      "48/48 [==============================] - 0s 817us/step - loss: 0.5392 - accuracy: 0.7253\n",
      "Epoch 83/150\n",
      "48/48 [==============================] - 0s 783us/step - loss: 0.5418 - accuracy: 0.7266\n",
      "Epoch 84/150\n",
      "48/48 [==============================] - 0s 736us/step - loss: 0.5233 - accuracy: 0.7500\n",
      "Epoch 85/150\n",
      "48/48 [==============================] - 0s 831us/step - loss: 0.5423 - accuracy: 0.7526\n",
      "Epoch 86/150\n",
      "48/48 [==============================] - 0s 829us/step - loss: 0.5616 - accuracy: 0.7253\n",
      "Epoch 87/150\n",
      "48/48 [==============================] - 0s 751us/step - loss: 0.5358 - accuracy: 0.7383\n",
      "Epoch 88/150\n",
      "48/48 [==============================] - 0s 820us/step - loss: 0.5395 - accuracy: 0.7292\n",
      "Epoch 89/150\n",
      "48/48 [==============================] - 0s 871us/step - loss: 0.5695 - accuracy: 0.7266\n",
      "Epoch 90/150\n",
      "48/48 [==============================] - 0s 798us/step - loss: 0.5295 - accuracy: 0.7422\n",
      "Epoch 91/150\n",
      "48/48 [==============================] - 0s 767us/step - loss: 0.5400 - accuracy: 0.7409\n",
      "Epoch 92/150\n",
      "48/48 [==============================] - 0s 836us/step - loss: 0.5483 - accuracy: 0.7240\n",
      "Epoch 93/150\n",
      "48/48 [==============================] - 0s 760us/step - loss: 0.5291 - accuracy: 0.7513\n",
      "Epoch 94/150\n",
      "48/48 [==============================] - 0s 752us/step - loss: 0.5631 - accuracy: 0.7266\n",
      "Epoch 95/150\n",
      "48/48 [==============================] - 0s 851us/step - loss: 0.5240 - accuracy: 0.7487\n",
      "Epoch 96/150\n",
      "48/48 [==============================] - 0s 786us/step - loss: 0.5186 - accuracy: 0.7643\n",
      "Epoch 97/150\n",
      "48/48 [==============================] - 0s 706us/step - loss: 0.5693 - accuracy: 0.7305\n",
      "Epoch 98/150\n",
      "48/48 [==============================] - 0s 813us/step - loss: 0.5518 - accuracy: 0.7383\n",
      "Epoch 99/150\n",
      "48/48 [==============================] - 0s 740us/step - loss: 0.5210 - accuracy: 0.7656\n",
      "Epoch 100/150\n",
      "48/48 [==============================] - 0s 716us/step - loss: 0.5214 - accuracy: 0.7526\n",
      "Epoch 101/150\n",
      "48/48 [==============================] - 0s 800us/step - loss: 0.5609 - accuracy: 0.7305\n",
      "Epoch 102/150\n",
      "48/48 [==============================] - 0s 830us/step - loss: 0.5330 - accuracy: 0.7461\n",
      "Epoch 103/150\n",
      "48/48 [==============================] - 0s 806us/step - loss: 0.5212 - accuracy: 0.7435\n",
      "Epoch 104/150\n",
      "48/48 [==============================] - 0s 843us/step - loss: 0.5445 - accuracy: 0.7448\n",
      "Epoch 105/150\n",
      "48/48 [==============================] - 0s 793us/step - loss: 0.5588 - accuracy: 0.7435\n",
      "Epoch 106/150\n",
      "48/48 [==============================] - 0s 862us/step - loss: 0.5341 - accuracy: 0.7370\n",
      "Epoch 107/150\n",
      "48/48 [==============================] - 0s 852us/step - loss: 0.5157 - accuracy: 0.7500\n",
      "Epoch 108/150\n",
      "48/48 [==============================] - 0s 812us/step - loss: 0.5243 - accuracy: 0.7500\n",
      "Epoch 109/150\n",
      "48/48 [==============================] - 0s 770us/step - loss: 0.5583 - accuracy: 0.7422\n",
      "Epoch 110/150\n",
      "48/48 [==============================] - 0s 848us/step - loss: 0.5129 - accuracy: 0.7487\n",
      "Epoch 111/150\n",
      "48/48 [==============================] - 0s 789us/step - loss: 0.5211 - accuracy: 0.7526\n",
      "Epoch 112/150\n",
      "48/48 [==============================] - 0s 840us/step - loss: 0.5353 - accuracy: 0.7370\n",
      "Epoch 113/150\n",
      "48/48 [==============================] - 0s 849us/step - loss: 0.5209 - accuracy: 0.7604\n",
      "Epoch 114/150\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.5209 - accuracy: 0.7500\n",
      "Epoch 115/150\n",
      "48/48 [==============================] - 0s 898us/step - loss: 0.5350 - accuracy: 0.7435\n",
      "Epoch 116/150\n",
      "48/48 [==============================] - 0s 852us/step - loss: 0.5252 - accuracy: 0.7630\n",
      "Epoch 117/150\n",
      "48/48 [==============================] - 0s 799us/step - loss: 0.5766 - accuracy: 0.7122\n",
      "Epoch 118/150\n",
      "48/48 [==============================] - 0s 848us/step - loss: 0.5334 - accuracy: 0.7565\n",
      "Epoch 119/150\n",
      "48/48 [==============================] - 0s 856us/step - loss: 0.5346 - accuracy: 0.7474\n",
      "Epoch 120/150\n",
      "48/48 [==============================] - 0s 816us/step - loss: 0.5338 - accuracy: 0.7422\n",
      "Epoch 121/150\n",
      "48/48 [==============================] - 0s 708us/step - loss: 0.5134 - accuracy: 0.7552\n",
      "Epoch 122/150\n",
      "48/48 [==============================] - 0s 734us/step - loss: 0.5418 - accuracy: 0.7318\n",
      "Epoch 123/150\n",
      "48/48 [==============================] - 0s 779us/step - loss: 0.5245 - accuracy: 0.7422\n",
      "Epoch 124/150\n",
      "48/48 [==============================] - 0s 733us/step - loss: 0.5259 - accuracy: 0.7591\n",
      "Epoch 125/150\n",
      "48/48 [==============================] - 0s 814us/step - loss: 0.5447 - accuracy: 0.7344\n",
      "Epoch 126/150\n",
      "48/48 [==============================] - 0s 828us/step - loss: 0.5033 - accuracy: 0.7578\n",
      "Epoch 127/150\n",
      "48/48 [==============================] - 0s 820us/step - loss: 0.5146 - accuracy: 0.7500\n",
      "Epoch 128/150\n",
      "48/48 [==============================] - 0s 720us/step - loss: 0.5451 - accuracy: 0.7487\n",
      "Epoch 129/150\n",
      "48/48 [==============================] - 0s 818us/step - loss: 0.5520 - accuracy: 0.7383\n",
      "Epoch 130/150\n",
      "48/48 [==============================] - 0s 785us/step - loss: 0.5445 - accuracy: 0.7396\n",
      "Epoch 131/150\n",
      "48/48 [==============================] - 0s 733us/step - loss: 0.5017 - accuracy: 0.7617\n",
      "Epoch 132/150\n",
      "48/48 [==============================] - 0s 802us/step - loss: 0.5120 - accuracy: 0.7656\n",
      "Epoch 133/150\n",
      "48/48 [==============================] - 0s 831us/step - loss: 0.5089 - accuracy: 0.7604\n",
      "Epoch 134/150\n",
      "48/48 [==============================] - 0s 849us/step - loss: 0.5437 - accuracy: 0.7435\n",
      "Epoch 135/150\n",
      "48/48 [==============================] - 0s 834us/step - loss: 0.5233 - accuracy: 0.7409\n",
      "Epoch 136/150\n",
      "48/48 [==============================] - 0s 873us/step - loss: 0.5185 - accuracy: 0.7643\n",
      "Epoch 137/150\n",
      "48/48 [==============================] - 0s 810us/step - loss: 0.5210 - accuracy: 0.7500\n",
      "Epoch 138/150\n",
      "48/48 [==============================] - 0s 876us/step - loss: 0.5247 - accuracy: 0.7578\n",
      "Epoch 139/150\n",
      "48/48 [==============================] - 0s 848us/step - loss: 0.4883 - accuracy: 0.7669\n",
      "Epoch 140/150\n",
      "48/48 [==============================] - 0s 826us/step - loss: 0.5001 - accuracy: 0.7695\n",
      "Epoch 141/150\n",
      "48/48 [==============================] - 0s 736us/step - loss: 0.5125 - accuracy: 0.7682\n",
      "Epoch 142/150\n",
      "48/48 [==============================] - 0s 860us/step - loss: 0.5192 - accuracy: 0.7500\n",
      "Epoch 143/150\n",
      "48/48 [==============================] - 0s 804us/step - loss: 0.5285 - accuracy: 0.7474\n",
      "Epoch 144/150\n",
      "48/48 [==============================] - 0s 836us/step - loss: 0.5045 - accuracy: 0.7565\n",
      "Epoch 145/150\n",
      "48/48 [==============================] - 0s 838us/step - loss: 0.4987 - accuracy: 0.7552\n",
      "Epoch 146/150\n",
      "48/48 [==============================] - 0s 820us/step - loss: 0.5262 - accuracy: 0.7578\n",
      "Epoch 147/150\n",
      "48/48 [==============================] - 0s 792us/step - loss: 0.5285 - accuracy: 0.7539\n",
      "Epoch 148/150\n",
      "48/48 [==============================] - 0s 843us/step - loss: 0.5474 - accuracy: 0.7409\n",
      "Epoch 149/150\n",
      "48/48 [==============================] - 0s 783us/step - loss: 0.5220 - accuracy: 0.7526\n",
      "Epoch 150/150\n",
      "48/48 [==============================] - 0s 832us/step - loss: 0.5552 - accuracy: 0.7305\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe6738d16a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=150, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb751d3",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "While we have an accuracy rating, it only applies to the training dataset. We do not have an out of sample dataset to work with. \n",
    "\n",
    "We can use the `evaluate()` function and pass it the same input/output used to train the model. This will generate a prediction for each input and output pair and collect scores, including the average loss and any metrics you have configured, such as accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6472b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 794us/step - loss: 0.5848 - accuracy: 0.7266\n",
      "Accuracy: 72.66\n"
     ]
    }
   ],
   "source": [
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X, y)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3df868",
   "metadata": {},
   "source": [
    "Neural networks are a stochastic algorithm, meaning that the same algorithm ont he same data can train a different model with different skill each time the code is run. This is a feature, not a bug. The variance in the performance of the model means that to get a reasomable approximation of how well your model is performing, you may need to fit it many times and calculate the average of the accuracy scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ab14ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_model_example(X=X, y=y, num=5):\n",
    "    accuracies = []\n",
    "    for i in range(num):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "        model.add(Dense(8, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        # compile the keras model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # fit the keras model on the dataset\n",
    "        model.fit(X, y, epochs=150, batch_size=16, verbose=0)\n",
    "        # evaluate the keras model\n",
    "        _, accuracy = model.evaluate(X, y)\n",
    "        accuracies.append(accuracy*100)\n",
    "    print(\"\\n\", f\"The average accuracy over {num} fits is: {round (sum(accuracies) / len(accuracies), 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13a3dd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 811us/step - loss: 0.4644 - accuracy: 0.7995\n",
      "24/24 [==============================] - 0s 901us/step - loss: 0.4923 - accuracy: 0.7643\n",
      "24/24 [==============================] - 0s 902us/step - loss: 0.5020 - accuracy: 0.7578\n",
      "24/24 [==============================] - 0s 797us/step - loss: 0.5498 - accuracy: 0.7318\n",
      "24/24 [==============================] - 0s 832us/step - loss: 0.5154 - accuracy: 0.7331\n",
      "\n",
      " The average accuracy over 5 fits is: 75.73%\n"
     ]
    }
   ],
   "source": [
    "multiple_model_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50be26a",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "\n",
    "We can adapt the above example and use it to generate predictions on the training dataset, pretending it is a new dataset we have not seen before. Making predictions is as easy as calling the `predict()` function on the model. We are using a sigmoid activation function on the output layer, so the predictions will be a probability in the range between 0 and 1. We can easily convert them into a crisp binary prediction for this classification task by rounding them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d76e4ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 602us/step\n"
     ]
    }
   ],
   "source": [
    "# make probability predictions with the model\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd8637e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing the first 5 predictions:\n",
      "[1, 0, 1, 0, 1]\n",
      "There are 768 predictions made for this set.\n"
     ]
    }
   ],
   "source": [
    "print(\"Showing the first 5 predictions:\")\n",
    "print(rounded[:5])\n",
    "print(f\"There are {len(rounded)} predictions made for this set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73e373f",
   "metadata": {},
   "source": [
    "We can compare the predictions to the actual values in the target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f22e478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0] => 1 (expected 1)\n",
      "[1.0, 85.0, 66.0, 29.0, 0.0, 26.6, 0.351, 31.0] => 0 (expected 0)\n",
      "[8.0, 183.0, 64.0, 0.0, 0.0, 23.3, 0.672, 32.0] => 1 (expected 1)\n",
      "[1.0, 89.0, 66.0, 23.0, 94.0, 28.1, 0.167, 21.0] => 0 (expected 0)\n",
      "[0.0, 137.0, 40.0, 35.0, 168.0, 43.1, 2.288, 33.0] => 1 (expected 1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"%s => %d (expected %d)\" % (X[i].tolist(), rounded[i], y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40402087",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this example, we loaded the data from the Pima Indians Diabetes Onset dataset. We defined a neural network model in Keras. We compiled the model using the efficient numerical backend. We trained a model on the data. Then, we evaluated our model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
