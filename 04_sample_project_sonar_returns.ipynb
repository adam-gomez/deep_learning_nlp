{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c81981c",
   "metadata": {},
   "source": [
    "# Sample Project: Sonar Return Data\n",
    "\n",
    "In this notebook, we will explore a popular classification problem and several potential solutions using neural networks.\n",
    "\n",
    "## Classification of Sonar Returns\n",
    "\n",
    "This is a dataset that describes sonar chirp returns bouncing off of different surfaces. The 60 input variables are the strength of the returns at different angles. The goal is to differentiate sonar bounces from rocks from metal cylinders. All of variables are continuous and generally in the range of 0 to 1. The target variable is in a string format, with `M` for metal cylinders and `R` for rock. \n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f6d2375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee58555e",
   "metadata": {},
   "source": [
    "## Loading and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a8af537",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sonar.csv', header=None)\n",
    "dataset = df.values\n",
    "\n",
    "# split into input and output variables\n",
    "X = dataset[:,:60].astype(float)\n",
    "y = dataset[:,60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b5649d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02  , 0.0371, 0.0428, 0.0207, 0.0954, 0.0986, 0.1539, 0.1601,\n",
       "        0.3109, 0.2111, 0.1609, 0.1582, 0.2238, 0.0645, 0.066 , 0.2273,\n",
       "        0.31  , 0.2999, 0.5078, 0.4797, 0.5783, 0.5071, 0.4328, 0.555 ,\n",
       "        0.6711, 0.6415, 0.7104, 0.808 , 0.6791, 0.3857, 0.1307, 0.2604,\n",
       "        0.5121, 0.7547, 0.8537, 0.8507, 0.6692, 0.6097, 0.4943, 0.2744,\n",
       "        0.051 , 0.2834, 0.2825, 0.4256, 0.2641, 0.1386, 0.1051, 0.1343,\n",
       "        0.0383, 0.0324, 0.0232, 0.0027, 0.0065, 0.0159, 0.0072, 0.0167,\n",
       "        0.018 , 0.0084, 0.009 , 0.0032],\n",
       "       [0.0453, 0.0523, 0.0843, 0.0689, 0.1183, 0.2583, 0.2156, 0.3481,\n",
       "        0.3337, 0.2872, 0.4918, 0.6552, 0.6919, 0.7797, 0.7464, 0.9444,\n",
       "        1.    , 0.8874, 0.8024, 0.7818, 0.5212, 0.4052, 0.3957, 0.3914,\n",
       "        0.325 , 0.32  , 0.3271, 0.2767, 0.4423, 0.2028, 0.3788, 0.2947,\n",
       "        0.1984, 0.2341, 0.1306, 0.4182, 0.3835, 0.1057, 0.184 , 0.197 ,\n",
       "        0.1674, 0.0583, 0.1401, 0.1628, 0.0621, 0.0203, 0.053 , 0.0742,\n",
       "        0.0409, 0.0061, 0.0125, 0.0084, 0.0089, 0.0048, 0.0094, 0.0191,\n",
       "        0.014 , 0.0049, 0.0052, 0.0044],\n",
       "       [0.0262, 0.0582, 0.1099, 0.1083, 0.0974, 0.228 , 0.2431, 0.3771,\n",
       "        0.5598, 0.6194, 0.6333, 0.706 , 0.5544, 0.532 , 0.6479, 0.6931,\n",
       "        0.6759, 0.7551, 0.8929, 0.8619, 0.7974, 0.6737, 0.4293, 0.3648,\n",
       "        0.5331, 0.2413, 0.507 , 0.8533, 0.6036, 0.8514, 0.8512, 0.5045,\n",
       "        0.1862, 0.2709, 0.4232, 0.3043, 0.6116, 0.6756, 0.5375, 0.4719,\n",
       "        0.4647, 0.2587, 0.2129, 0.2222, 0.2111, 0.0176, 0.1348, 0.0744,\n",
       "        0.013 , 0.0106, 0.0033, 0.0232, 0.0166, 0.0095, 0.018 , 0.0244,\n",
       "        0.0316, 0.0164, 0.0095, 0.0078]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5581529a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['R', 'R', 'R'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcadd2e",
   "metadata": {},
   "source": [
    "The output variable is a string datatype. Previously we converted this to one-hot encoded representation using `pd.get_dummies().to_numpy()`. This time we will use the `LabelEncoder` class from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbd38c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_Y = encoder.transform(y)\n",
    "encoded_Y[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac65236",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55b7fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_shape=(60,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7629a5d6",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f717279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 78.21428571428571% (Std: 9.952979020535507%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate model with dataset\n",
    "estimator = KerasClassifier(model=create_baseline, epochs=100, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(estimator, X, encoded_Y, cv=kfold)\n",
    "print(f\"Baseline: {results.mean()*100}% (Std: {results.std()*100}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0632ab1",
   "metadata": {},
   "source": [
    "## Improving Performance: Standardization\n",
    "\n",
    "While there are an incredible number of possible hyperparameter settings, we can approach improving the model through a more efficient method: data preparation. Neural network models are especially suitable to having consistent input values, both in scale and distribution. Scaling can standardize our data for improved performance. We can use scikit-learn to perform the standardization of our data using the `StandardScaler` class. \n",
    "\n",
    "> Note: Rather than performing the standardization on the entire dataset, it is good practice to train the standardization procedure on the training data **within the pass of a cross-validation run** and to use the trained standardization instance to prepare the unseen test fold. This ensures that standardization is a step in model preparation during cross-validation and prevents learning from out-of-sample data. \n",
    "\n",
    "We can achieve this using a `Pipeline` class. The pipeline is a wrapper that executes one or more models within a pass of the cross-validation procedure. Here we can define a pipeline with the `StandardScaler` followed by our neural network model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f91f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a01496f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: 85.57% (5.27%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate baseline model with a standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(model=create_baseline, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658bb881",
   "metadata": {},
   "source": [
    "After standardization, we have about a 5% improvement in accuracy and the standard deviation has been cut in half. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeaba3b",
   "metadata": {},
   "source": [
    "## Improving Performance: Network Topology\n",
    "\n",
    "We will run two experiments on the structure of the network. First, making it smaller, then making it larger. \n",
    "\n",
    "### A Smaller Network\n",
    "\n",
    "In the previous experiment, our network topology included 60 neurons in the hidden layer. In a sense, there is one hidden neuron for each input neuron. What if there were fewer? In this case, the network would be pressured to pick out the most important structure in the input data. Perhaps some angles are more important than others, and with fewer neurons to handle those patterns, we can end up with a better result. There's no way to know until we try it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edb77c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_smaller():\n",
    "    # create a smaller model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, input_shape=(60,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8aeaa578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized/Smaller: 84.69% (7.55%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate baseline model with a standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(model=create_smaller, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Standardized/Smaller: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab945734",
   "metadata": {},
   "source": [
    "In this case, it doesn't seem like the smaller network creates a meaningful difference. The two results are comparable, at least when other hyperparameters stay the same. One big advantage to the smaller network is the reduction in training time. If the two models are comparable in performance, then perhaps the smaller topology is ideal to use when evaluating other hyperparameters. \n",
    "\n",
    "### A Larger Network\n",
    "\n",
    "Now we can consider the impact of a larger network. Additional layers may better extract key features in non-linear ways. There are several ways to increase the size of the network topology. In this experiment, we will add another hidden layer to the original design. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edc92e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_larger():\n",
    "    # create a larger model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_shape=(60,), activation='relu'))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc71c0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized/Larger: 86.50% (6.83%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate baseline model with a standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(model=create_larger, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Standardized/Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0ff6df",
   "metadata": {},
   "source": [
    "Like the previous example, there doesn't appear to be a meaningful difference with the additional layer. Nevertheless, an infinite number of variations in network topology can be explored. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
