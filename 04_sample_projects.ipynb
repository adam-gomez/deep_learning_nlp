{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e2f9276",
   "metadata": {},
   "source": [
    "# Sample Projects\n",
    "\n",
    "In this notebook, we will explore several classification problems and a potential solution for each, using TensorFlow/Keras. \n",
    "\n",
    "## Classification of Sonar Returns\n",
    "\n",
    "This is a dataset that describes sonar chirp returns bouncing off of different surfaces. The 60 input variables are the strength of the returns at different angles. The goal is to differentiate sonar bounces from rocks from metal cylinders. All of variables are continuous and generally in the range of 0 to 1. The target variable is in a string format, with `M` for metal cylinders and `R` for rock. \n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4379558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17373a4c",
   "metadata": {},
   "source": [
    "## Loading and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5c484fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sonar.csv', header=None)\n",
    "dataset = df.values\n",
    "\n",
    "# split into input and output variables\n",
    "X = dataset[:,:60].astype(float)\n",
    "y = dataset[:,60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b48a7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02  , 0.0371, 0.0428, 0.0207, 0.0954, 0.0986, 0.1539, 0.1601,\n",
       "        0.3109, 0.2111, 0.1609, 0.1582, 0.2238, 0.0645, 0.066 , 0.2273,\n",
       "        0.31  , 0.2999, 0.5078, 0.4797, 0.5783, 0.5071, 0.4328, 0.555 ,\n",
       "        0.6711, 0.6415, 0.7104, 0.808 , 0.6791, 0.3857, 0.1307, 0.2604,\n",
       "        0.5121, 0.7547, 0.8537, 0.8507, 0.6692, 0.6097, 0.4943, 0.2744,\n",
       "        0.051 , 0.2834, 0.2825, 0.4256, 0.2641, 0.1386, 0.1051, 0.1343,\n",
       "        0.0383, 0.0324, 0.0232, 0.0027, 0.0065, 0.0159, 0.0072, 0.0167,\n",
       "        0.018 , 0.0084, 0.009 , 0.0032],\n",
       "       [0.0453, 0.0523, 0.0843, 0.0689, 0.1183, 0.2583, 0.2156, 0.3481,\n",
       "        0.3337, 0.2872, 0.4918, 0.6552, 0.6919, 0.7797, 0.7464, 0.9444,\n",
       "        1.    , 0.8874, 0.8024, 0.7818, 0.5212, 0.4052, 0.3957, 0.3914,\n",
       "        0.325 , 0.32  , 0.3271, 0.2767, 0.4423, 0.2028, 0.3788, 0.2947,\n",
       "        0.1984, 0.2341, 0.1306, 0.4182, 0.3835, 0.1057, 0.184 , 0.197 ,\n",
       "        0.1674, 0.0583, 0.1401, 0.1628, 0.0621, 0.0203, 0.053 , 0.0742,\n",
       "        0.0409, 0.0061, 0.0125, 0.0084, 0.0089, 0.0048, 0.0094, 0.0191,\n",
       "        0.014 , 0.0049, 0.0052, 0.0044],\n",
       "       [0.0262, 0.0582, 0.1099, 0.1083, 0.0974, 0.228 , 0.2431, 0.3771,\n",
       "        0.5598, 0.6194, 0.6333, 0.706 , 0.5544, 0.532 , 0.6479, 0.6931,\n",
       "        0.6759, 0.7551, 0.8929, 0.8619, 0.7974, 0.6737, 0.4293, 0.3648,\n",
       "        0.5331, 0.2413, 0.507 , 0.8533, 0.6036, 0.8514, 0.8512, 0.5045,\n",
       "        0.1862, 0.2709, 0.4232, 0.3043, 0.6116, 0.6756, 0.5375, 0.4719,\n",
       "        0.4647, 0.2587, 0.2129, 0.2222, 0.2111, 0.0176, 0.1348, 0.0744,\n",
       "        0.013 , 0.0106, 0.0033, 0.0232, 0.0166, 0.0095, 0.018 , 0.0244,\n",
       "        0.0316, 0.0164, 0.0095, 0.0078]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d08502a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['R', 'R', 'R'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b57986f",
   "metadata": {},
   "source": [
    "The output variable is a string datatype. Previously we converted this to one-hot encoded representation using `pd.get_dummies().to_numpy()`. This time we will use the `LabelEncoder` class from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbf2ad16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_Y = encoder.transform(y)\n",
    "encoded_Y[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ecf60d",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca885d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_shape=(60,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befec5e0",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49cfccb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 82.28571428571428% (Std: 9.238708865236452%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate model with dataset\n",
    "estimator = KerasClassifier(model=create_baseline, epochs=100, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(estimator, X, encoded_Y, cv=kfold)\n",
    "print(f\"Baseline: {results.mean()*100}% (Std: {results.std()*100}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad061a4f",
   "metadata": {},
   "source": [
    "## Improving Performance: Standardization\n",
    "\n",
    "While there are an incredible number of possible hyperparameter settings, we can approach improving the model through a more efficient method: data preparation. Neural network models are especially suitable to having consistent input values, both in scale and distribution. Scaling can standardize our data for improved performance. We can use scikit-learn to perform the standardization of our data using the `StandardScaler` class. \n",
    "\n",
    "> Note: Rather than performing the standardization on the entire dataset, it is good practice to train the standardization procedure on the training data **within the pass of a cross-validation run** and to use the trained standardization instance to prepare the unseen test fold. This ensures that standardization is a step in model preparation during cross-validation and prevents learning from out-of-sample data. \n",
    "\n",
    "We can achieve this using a `Pipeline` class. The pipeline is a wrapper that executes one or more models within a pass of the cross-validation procedure. Here we can define a pipeline with the `StandardScaler` followed by our neural network model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ddfd963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "551d3cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: 87.02% (4.28%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate baseline model with a standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(model=create_baseline, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d94afa9",
   "metadata": {},
   "source": [
    "After standardization, we have about a 5% improvement in accuracy and the standard deviation has been cut in half. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161c53a7",
   "metadata": {},
   "source": [
    "## Improving Performance: Network Topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4492517",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
